{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport Utils\n",
    "%aimport MatrixLinkGenerator\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "np.set_printoptions(edgeitems=30, linewidth=100000, formatter=dict(float=lambda x: \"%.3g\" % x))\n",
    "# np.set_printoptions(linewidth=np.inf)\n",
    "from obspy import UTCDateTime as dt\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from Utils import trainingResults, trainingResults2, predsMap\n",
    "# plt.rcParams['figure.figsize'] = [50, 200]\n",
    "plt.rcParams['figure.figsize'] = [16, 12]\n",
    "params = json.loads('''{\n",
    "    \"extents\": {\n",
    "        \"ak\": {\n",
    "            \"latMin\": 55.0,\n",
    "            \"latMax\": 74.0,\n",
    "            \"lonMin\": -163.0,\n",
    "            \"lonMax\": -130.0\n",
    "        },\n",
    "        \"s1\": {\n",
    "            \"latMin\": 22.0,\n",
    "            \"latMax\": 40.0,\n",
    "            \"lonMin\": 33.0,\n",
    "            \"lonMax\": 62.0\n",
    "        },\n",
    "        \"global\": {\n",
    "            \"latMin\": -90.0,\n",
    "            \"latMax\": 90.0,\n",
    "            \"lonMin\": -180.0,\n",
    "            \"lonMax\": 180.0\n",
    "        }\n",
    "    },\n",
    "    \"location\": \"global\",\n",
    "    \"maxDepth\": 50.0,\n",
    "    \"maxStationElevation\": 1.0,\n",
    "    \"trainingGeneratorSourceFile\": \"./Inputs/IDC 10-20.gz\",\n",
    "    \"trainingEventsFile\": \"./Training/Event Files/IDC 10-20 ECEF Stations Times.npz\",\n",
    "    \"validationGeneratorSourceFile\": \"./Inputs/IDC 10-20.gz\",\n",
    "    \"validationEventsFile\": \"./Training/Event Files/IDC 10-20 ECEF Stations Times.npz\",\n",
    "    \"arrivalProbsFile\": \"./Training/RSTT Model/S1 Dropouts.npy\",\n",
    "    \"stationFile\": \"./Archive/Stations/S1 Station List.txt\",\n",
    "    \"oneHot\": \"True\",\n",
    "    \"arrivalProbMods\": {\n",
    "        \"Pg\": 5.0,\n",
    "        \"Pn\": 3.0,\n",
    "        \"Sg\": 5.0,\n",
    "        \"Sn\": 25.0\n",
    "    },\n",
    "    \"eventsPerExample\": {\n",
    "        \"min\": 6,\n",
    "        \"max\": 20\n",
    "    },\n",
    "    \"stationsPerBatch\": {\n",
    "        \"min\": 45,\n",
    "        \"max\": 55\n",
    "    },\n",
    "    \"timeShifts\": {\n",
    "        \"min\": -0.20,\n",
    "        \"max\": 0.40\n",
    "    },\n",
    "    \"batchSize\": 500,\n",
    "    \"samplesPerEpoch\": 1000000,\n",
    "    \"validationSamplesPerEpoch\": 250000,\n",
    "    \"epochs\": 1000,\n",
    "    \"model\": \"./Training/Models/IDC/E054 L0.0017 AL0.0009 LL172.8020 TL0.0002 AA0.1080 AP0.8677 AR0.7861.h5\",\n",
    "    \"evalInFile\": \"./Inputs/S1 00.gz\",\n",
    "    \"evalOutFile\": \"./Training/Evaluation.gz\",\n",
    "    \"prlEvalOutFile\": \"./Training/PRL Evaluation.gz\",\n",
    "    \"maxArrivals\": 250,\n",
    "    \"minArrivals\": 5,\n",
    "    \"maxNoise\": 0.20,\n",
    "    \"clusterStrength\": 0.9,\n",
    "    \"timeNormalize\": 3600,\n",
    "    \"associationWindow\": 3600,\n",
    "    \"evalWindow\": 10.0,\n",
    "    \"phases\": {\n",
    "        \"P\": 0,\n",
    "        \"LR\": 1,\n",
    "        \"Pn\": 2,\n",
    "        \"T\": 3,\n",
    "        \"tx\": 4,\n",
    "        \"N\": 5,\n",
    "        \"Sx\": 6,\n",
    "        \"Pg\": 7,\n",
    "        \"Lg\": 8,\n",
    "        \"Sn\": 9,\n",
    "        \"S\": 10\n",
    "    },\n",
    "    \"modelArch\": {\n",
    "        \"dense\": [32, 32, 32, 64, 64],\n",
    "        \"transformers\": [128, 128],\n",
    "        \"heads\": 4,\n",
    "        \"dense2\": [128, 128, 128],\n",
    "        \"grus\": [128, 128]\n",
    "    }\n",
    "}''')\n",
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import rstt\n",
    "from copy import deepcopy\n",
    "from collections import deque\n",
    "modelPath = \"./Training/RSTT Model/pdu202009Du.geotess\"\n",
    "\n",
    "def generateEventFile(params, trainingSet = False):\n",
    "    if trainingSet:\n",
    "        eventsFile = params['trainingEventsFile']\n",
    "        generatorFile = params['trainingGeneratorSourceFile']\n",
    "    else:\n",
    "        eventsFile = params['validationEventsFile']\n",
    "        generatorFile = params['validationGeneratorSourceFile']\n",
    "    try:\n",
    "        evFile = np.load(eventsFile, allow_pickle=True)\n",
    "        events = evFile['events'].flatten()[0]\n",
    "        eventsList = evFile['eventsList']\n",
    "        print(\"Training events loaded.\") if trainingSet else print(\"Validation events loaded.\")\n",
    "    except:\n",
    "        print(\"Events not loaded. Building from scratch.\")\n",
    "        extents = np.array(list(params['extents'][params['location']].values())+[params['maxDepth'],params['maxStationElevation']])\n",
    "        latRange = abs(extents[1] - extents[0])\n",
    "        lonRange = abs(extents[3] - extents[2])\n",
    "        timeNormalize = params['timeNormalize']\n",
    "        phases = params['phases']\n",
    "        events = {}\n",
    "        eventsList = []\n",
    "        inputArrivals = pd.read_pickle(generatorFile).sort_values(by=\"TIME\")\n",
    "        groupedEvents = (inputArrivals.groupby('EVID').filter(lambda x: len(x) >= params['minArrivals'])).groupby('EVID')\n",
    "        count = 0\n",
    "        for eid, arrivals in groupedEvents:\n",
    "            count += 1\n",
    "            print(\"\\rBuilding event list: \" + str(count) + ' / ' + str(len(groupedEvents)), end='')\n",
    "            eventArrivals = []\n",
    "            first = dt(arrivals.TIME.min())\n",
    "            evtime = -(first - dt(arrivals.EV_TIME.min())) / timeNormalize\n",
    "            for i, arrival in arrivals.iterrows():\n",
    "                sx,sy,sz = ll2ecef(arrival.ST_LAT, arrival.ST_LON)\n",
    "                thisArrival = [sx,                                        # normalized station x\n",
    "                               sy,                                        # normalized station y\n",
    "                               sz,                                        # normalized station z\n",
    "                               ((dt(arrival.TIME)-first)/timeNormalize),  # normalized arrival time\n",
    "                               phases[arrival.PHASE],                     # phase\n",
    "                               1.,                                        # valid arrival flag\n",
    "                               5.0 / timeNormalize,                       # arrival uncertainty\n",
    "                               0.9,                                       # retention rate when dropping some arrivals\n",
    "                               abs((arrival.EV_LAT-extents[0])/latRange), # normalized event lat\n",
    "                               abs((arrival.EV_LON-extents[2])/lonRange), # normalized event lon\n",
    "                               evtime]                                    # normalized event time (relative to first arrival)\n",
    "                eventArrivals.append(thisArrival)\n",
    "            events[eid] = np.array(eventArrivals)\n",
    "            eventsList.append(np.array([eid,arrivals.TIME.min()]))\n",
    "        eventsList = np.array(eventsList)\n",
    "        eventsList = eventsList[np.argsort(eventsList[:,1])]\n",
    "        np.savez_compressed(eventsFile, events=events, eventsList=eventsList)\n",
    "        print()\n",
    "    return events, eventsList\n",
    "\n",
    "def buildAssociationMatrix(evids):\n",
    "    L = np.zeros((len(evids), len(evids))) + 99\n",
    "    sparse_evids = evids[evids>=0]\n",
    "    l = np.ones((len(sparse_evids), len(sparse_evids))) * sparse_evids.reshape((-1, 1))\n",
    "    L[:len(sparse_evids), :len(sparse_evids)] = (l == l.T) * 1\n",
    "    return L\n",
    "\n",
    "def synthesizeEventsFromEventFile(params, events, eventList, trainingSet = False):\n",
    "    maxArrivals = params['maxArrivals']\n",
    "    minTimeShift = params['timeShifts']['min']\n",
    "    maxTimeShift = params['timeShifts']['max']\n",
    "    minEvents = params['eventsPerExample']['min']\n",
    "    maxEvents = params['eventsPerExample']['max']+1 # because using in np.random.randint\n",
    "    dropFactor = 0.5\n",
    "    batchSize = params['batchSize']\n",
    "    start = eventList[0][1]\n",
    "    end = eventList[-1][1]\n",
    "    time = start\n",
    "\n",
    "    while True:\n",
    "        X = []\n",
    "        Y = []\n",
    "        example = 0\n",
    "        while example < batchSize:\n",
    "            #Setup - choose random events, with the first being the primary event\n",
    "#             numEvents = np.random.randint(minEvents, maxEvents)\n",
    "#             chosenEvents = random.sample(eventList, numEvents)\n",
    "\n",
    "            chosenEvents = np.where((trainingEventList[:,1] >= time) & (trainingEventList[:,1] <= time+timeNormalize))[0]\n",
    "            numEvents = len(chosenEvents)\n",
    "\n",
    "#             timeShifts = np.random.uniform(minTimeShift, maxTimeShift, size=numEvents-1)\n",
    "            for i in range(0, len(chosenEvents)):\n",
    "                thisEvent = events[eventList[chosenEvents[i]][0]]\n",
    "                #Randomly drop some picks from the event\n",
    "                if trainingSet:\n",
    "                    drops = thisEvent[:,7]\n",
    "                    drops = drops + dropFactor*(1-drops) if dropFactor > 0 else drops*(1+dropFactor)\n",
    "                    drops = np.random.binomial(1,drops)\n",
    "                    idx = np.where(drops==1)[0]\n",
    "                    thisEvent = thisEvent[idx,:]\n",
    "                if i == 0:\n",
    "                    sequence = thisEvent\n",
    "                    sequence[:,5] = i\n",
    "                else:\n",
    "                    #Add the picks from this event\n",
    "                    currentLength = len(sequence)\n",
    "                    sequence = np.append(sequence, thisEvent, axis=0)\n",
    "                    #Shift the starting time of this event\n",
    "#                     sequence[currentLength:currentLength+len(thisEvent),[3,10]] += timeShifts[i-1]\n",
    "                    sequence[currentLength:currentLength+len(thisEvent),[3,10]] += ((eventList[chosenEvents[i]][1] - time)/timeNormalize)\n",
    "                    sequence[currentLength:currentLength+len(thisEvent),5] = i\n",
    "            if time > end:\n",
    "                time = start\n",
    "            else:\n",
    "                time += timeNormalize\n",
    "            if numEvents == 0:\n",
    "                continue\n",
    "        \n",
    "            #Add random arrival time errors, except for the first pick of the primary event\n",
    "            if trainingSet:\n",
    "                timeShifts = np.random.uniform(-sequence[1:,6]*0.5, sequence[1:,6]*0.5)\n",
    "                sequence[1:,3] += timeShifts\n",
    "\n",
    "            #Sort by arrival time, drop picks with negative arrival times\n",
    "            idx = np.argsort(sequence[:,3])\n",
    "            remove = len(np.where((sequence[:,3] < 0))[0])\n",
    "            idx = idx[remove:]\n",
    "            sequence = sequence[idx,:]\n",
    "            if len(sequence) == 0: #We lost all the valid arrivals, so scrap this training example\n",
    "                continue\n",
    "\n",
    "            #Make labels array\n",
    "            labels = buildAssociationMatrix(sequence[:,5])\n",
    "\n",
    "            #Reset primary event times\n",
    "            ones = np.where(labels[0]==1)\n",
    "#             if len(ones[0]) == 0: #We lost all the valid arrivals, so scrap this training example\n",
    "#                 continue\n",
    "            sequence[ones,3] -= sequence[ones,3][0][0]\n",
    "            idx = np.argsort(sequence[:,3])\n",
    "\n",
    "            #Truncate picks over maximum allowed\n",
    "            idx = idx[:maxArrivals]\n",
    "            sequence = sequence[idx,:]\n",
    "            labels = labels[idx,:maxArrivals]\n",
    "\n",
    "            sequence[:,5] = 1.\n",
    "            #Pad the end if not enough picks were selected\n",
    "            padding = maxArrivals - len(sequence)\n",
    "            if padding > 0:\n",
    "                labels = np.pad(labels, (0, maxArrivals-len(labels)), constant_values=99.)\n",
    "                sequence_ = np.full((maxArrivals, 11), 0.)\n",
    "#                 sequence_[sequence.shape[0]:, [8,9]] = 99.\n",
    "                sequence_[:sequence.shape[0], :] = sequence\n",
    "                sequence = sequence_\n",
    "            X.append(sequence)\n",
    "            Y.append(labels)\n",
    "            example += 1\n",
    "            \n",
    "        #Yield these training examples\n",
    "        X = np.array(X)\n",
    "        Y = {\"association\": np.array(Y), \"location\": X[:,:,[8,9]], \"time\": X[:,:,10]}\n",
    "        X = {\"phase\": X[:,:,4], \"numerical_features\": X[:,:,[0,1,2,3,5]]}\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxArrivals = params['maxArrivals']\n",
    "from tensorflow.keras import backend as K\n",
    "from geographiclib.geodesic import Geodesic\n",
    "from tensorflow.keras.losses import binary_crossentropy as BCE\n",
    "from tensorflow.keras.metrics import binary_accuracy\n",
    "\n",
    "K.set_floatx('float64')\n",
    "maxArrivals = params['maxArrivals']\n",
    "matrixSize = maxArrivals**2\n",
    "extents = np.array(list(params['extents'][params['location']].values())+[params['maxDepth'],params['maxStationElevation']])\n",
    "latRange = abs(extents[1] - extents[0])\n",
    "lonRange = abs(extents[3] - extents[2])\n",
    "timeNormalize = params['timeNormalize']\n",
    "zero = np.float64(0)\n",
    "one = np.float64(1)\n",
    "r = np.float64(12756.2)\n",
    "nn = np.float64(99)\n",
    "\n",
    "import pymap3d as pm\n",
    "xym = 6378137.0\n",
    "xym2 = 2*xym\n",
    "zm = 6356752.3142451802\n",
    "zm2 = 2*zm\n",
    "def ll2ecef(lat,lon):\n",
    "    x,y,z = pm.geodetic2ecef(lat, lon, 0)\n",
    "    x = (x+xym)/xym2\n",
    "    y = (y+xym)/xym2\n",
    "    z = (z+zm)/zm2\n",
    "    return x,y,z\n",
    "def ecef2ll(x,y,z):\n",
    "    x = x*xym2 - xym\n",
    "    y = y*xym2 - xym\n",
    "    z = z*zm2 - zm\n",
    "    x,y,z = pm.ecef2geodetic(x,y,z)\n",
    "    return x,y\n",
    "\n",
    "def nzHaversine(y_true, y_pred):\n",
    "#     y_pred = y_pred * tf.cast(y_true != nn, tf.float64)\n",
    "#     y_true = y_true * tf.cast(y_true != nn, tf.float64)\n",
    "    observation = tf.stack([y_true[:,:,0]*latRange + extents[0], y_true[:,:,1]*lonRange + extents[2]],axis=2)*0.017453292519943295\n",
    "    prediction = tf.stack([y_pred[:,:,0]*latRange + extents[0], y_pred[:,:,1]*lonRange + extents[2]],axis=2)*0.017453292519943295\n",
    "    used = tf.reduce_sum(tf.cast(tf.greater(tf.reduce_sum(y_true, axis=2),0), dtype=tf.float64), axis=1)\n",
    "    used = tf.where(tf.equal(used, zero), one, used)\n",
    "    dlat_dlon = (observation - prediction) / 2\n",
    "    a = tf.sin(dlat_dlon[:,:,0])**2 + tf.cos(observation[:,:,0]) * tf.cos(prediction[:,:,0]) * tf.sin(dlat_dlon[:,:,1])**2\n",
    "    c = tf.asin(tf.sqrt(a))*r\n",
    "    return tf.reduce_sum((tf.reduce_sum(c, axis=1))/used) / tf.dtypes.cast(tf.shape(observation)[0], dtype=tf.float64)\n",
    "\n",
    "def nzTime(y_true, y_pred):\n",
    "    y_pred = y_pred * tf.cast(y_true != nn, tf.float64)\n",
    "    y_true = y_true * tf.cast(y_true != nn, tf.float64)\n",
    "    used = maxArrivals - tf.reduce_sum(tf.cast(tf.equal(y_true, zero), dtype=tf.float64), axis=1)\n",
    "    used = tf.where(tf.equal(used, zero), one, used)\n",
    "    diffs = tf.math.abs(tf.squeeze(y_pred)-y_true)*timeNormalize\n",
    "    diffs = (tf.squeeze(y_pred)-y_true)*timeNormalize\n",
    "    diffs = tf.reduce_sum(tf.reduce_sum(diffs, axis=1)/used)\n",
    "    return diffs/tf.dtypes.cast(tf.shape(y_true)[0], dtype= tf.float64)\n",
    "\n",
    "def nzMSE(y_true, y_pred):\n",
    "    y_pred = y_pred * tf.cast(y_true != nn, tf.float64)\n",
    "    y_true = y_true * tf.cast(y_true != nn, tf.float64)\n",
    "    used = maxArrivals - tf.reduce_sum(tf.cast(tf.equal(y_true,0), dtype=tf.float64), axis=1)\n",
    "    used = tf.where(tf.equal(used, zero), one, used)\n",
    "    return K.mean(tf.reduce_sum(K.square(tf.squeeze(y_pred)-y_true),axis=1)/used)\n",
    "\n",
    "def nzBCE(y_true, y_pred):\n",
    "    y_pred = y_pred * tf.cast(y_true != nn, tf.float64)\n",
    "    y_true = y_true * tf.cast(y_true != nn, tf.float64)\n",
    "    used = maxArrivals - tf.reduce_sum(tf.cast(tf.equal(y_true,0), dtype=tf.float64), axis=1)\n",
    "    used = tf.where(tf.equal(used, zero), one, used)\n",
    "    return K.mean(BCE(y_true, y_pred)/used)\n",
    "\n",
    "def nzAccuracy(y_true, y_pred):\n",
    "    y_pred = tf.squeeze(y_pred) * tf.cast(y_true != nn, tf.float64)\n",
    "    y_true = y_true * tf.cast(y_true != nn, tf.float64)\n",
    "    return K.mean(binary_accuracy(y_true, y_pred))\n",
    "\n",
    "def nzRecall(y_true, y_pred):\n",
    "    y_pred = y_pred * tf.cast(y_true != nn, tf.float64)\n",
    "    y_true = y_true * tf.cast(y_true != nn, tf.float64)\n",
    "#     y_true = K.ones_like(y_true)\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    all_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    return true_positives / (all_positives + K.epsilon())\n",
    "\n",
    "def nzPrecision(y_true, y_pred):\n",
    "    y_pred = y_pred * tf.cast(y_true != nn, tf.float64)\n",
    "    y_true = y_true * tf.cast(y_true != nn, tf.float64)\n",
    "#     y_true = K.ones_like(y_true)\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    \n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    return true_positives / (predicted_positives + K.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#MatrixLinkTrainer\n",
    "import tensorflow as tf\n",
    "import absl.logging\n",
    "absl.logging.set_verbosity(absl.logging.ERROR)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.layers import Input, Embedding, Reshape, concatenate, Dense, Bidirectional, GRU, MultiHeadAttention, LayerNormalization, Lambda\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, CSVLogger\n",
    "from tensorflow.keras.backend import clip\n",
    "import logging\n",
    "import json\n",
    "\n",
    "@tf.autograph.experimental.do_not_convert\n",
    "def MatrixLink(params):\n",
    "    logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "    def buildModel(modelArch):\n",
    "        outputs = []\n",
    "        inputs = []\n",
    "        numericalInputs = Input(shape=(None,5), name='numerical_features')\n",
    "        outputs.append(numericalInputs)\n",
    "        inputs.append(numericalInputs)\n",
    "        categoricalInputs = Input(shape=(None,1), name='phase')\n",
    "        embed = Embedding(11, 4, trainable=True, embeddings_initializer=RandomNormal())(categoricalInputs)\n",
    "        embed = Reshape(target_shape=(-1, 4))(embed)\n",
    "        outputs.append(embed)\n",
    "        inputs.append(categoricalInputs)\n",
    "        outputs = concatenate(outputs)\n",
    "\n",
    "        def TransformerBlock(inputs, embed_dim, ff_dim, num_heads=2, rate=0.1, eps=1e-6):\n",
    "            attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(inputs, inputs)\n",
    "#             attn_output = Dropout(rate)(attn_output)\n",
    "            out1 = LayerNormalization(epsilon=eps)(inputs + attn_output)\n",
    "            ffn_output = Dense(ff_dim, activation=\"relu\")(out1)\n",
    "            ffn_output = Dense(embed_dim)(ffn_output)\n",
    "#             ffn_output = Dropout(rate)(ffn_output)\n",
    "            return LayerNormalization(epsilon=eps)(out1 + ffn_output) \n",
    "\n",
    "        for d1Units in modelArch['dense']:\n",
    "            outputs = Dense(units=d1Units, activation=tf.nn.relu)(outputs)\n",
    "        transformerOutputs = outputs\n",
    "        gruOutputs = outputs\n",
    "\n",
    "        for tUnits in modelArch['transformers']:\n",
    "            transformerOutputs = TransformerBlock(transformerOutputs, d1Units, tUnits, modelArch['heads'])\n",
    "        for gUnits in modelArch['grus']:\n",
    "            gruOutputs = Bidirectional(GRU(gUnits, return_sequences=True))(gruOutputs)\n",
    "\n",
    "        outputs = concatenate([transformerOutputs, gruOutputs], axis=2)\n",
    "        for tUnits in modelArch['transformers']:\n",
    "            outputs = TransformerBlock(outputs, d1Units+gUnits*2, tUnits, modelArch['heads'])\n",
    "\n",
    "        association = Dense(units=params['maxArrivals'], activation=tf.nn.sigmoid, name='association')(outputs)\n",
    "        location = Dense(units=2)(outputs)\n",
    "        location = Lambda(lambda x: clip(x, 0, 1), name='location')(location)\n",
    "        time = Dense(units=1, name='time')(outputs)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=[association, location, time])\n",
    "        losses = { 'association': nzBCE, 'location': nzHaversine, 'time': nzMSE }\n",
    "        weights = { 'association': 1.0, 'location': 0.000004, 'time': 0.25 }\n",
    "        metrics = { 'association': [nzAccuracy, nzPrecision, nzRecall] }\n",
    "        model.compile(optimizer=Adam(clipnorm=0.00001), loss=losses, loss_weights=weights, metrics=metrics)\n",
    "        return model\n",
    "\n",
    "    model = buildModel(params['modelArch'])\n",
    "    try:\n",
    "        model.load_weights(params['model'])\n",
    "        print(\"Loaded previous weights.\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"No previous weights loaded.\")\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "class saveCb(Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.best = 100000000.\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs['loss'] < self.best:\n",
    "            self.best = logs['loss']\n",
    "            print('Saving best model with loss', self.best)\n",
    "            modelName = 'E%03d L%.4f AL%.4f LL%.4f TL%.4f AA%.4f AP%.4f AR%.4f.h5' %\\\n",
    "                (epoch, logs['loss'], logs['association_loss'], logs['location_loss'], logs['time_loss'], logs['association_nzAccuracy'], logs['association_nzPrecision'], logs['association_nzRecall'])\n",
    "            model.save(\"./Training/Models/IDC/\"+modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training events loaded.\n",
      "Loaded previous weights.\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "phase (InputLayer)              [(None, None, 1)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 1, 4)   44          phase[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "numerical_features (InputLayer) [(None, None, 5)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, None, 4)      0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, None, 9)      0           numerical_features[0][0]         \n",
      "                                                                 reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 32)     320         concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 32)     1056        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, None, 32)     1056        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, None, 64)     2112        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, None, 64)     4160        dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention (MultiHead (None, None, 64)     66368       dense_4[0][0]                    \n",
      "                                                                 dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add (TFOpLambd (None, None, 64)     0           dense_4[0][0]                    \n",
      "                                                                 multi_head_attention[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization (LayerNorma (None, None, 64)     128         tf.__operators__.add[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, None, 128)    8320        layer_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, None, 64)     8256        dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_1 (TFOpLam (None, None, 64)     0           layer_normalization[0][0]        \n",
      "                                                                 dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (None, None, 64)     128         tf.__operators__.add_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_1 (MultiHe (None, None, 64)     66368       layer_normalization_1[0][0]      \n",
      "                                                                 layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_2 (TFOpLam (None, None, 64)     0           layer_normalization_1[0][0]      \n",
      "                                                                 multi_head_attention_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_2 (LayerNor (None, None, 64)     128         tf.__operators__.add_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, None, 128)    8320        layer_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, None, 64)     8256        dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_3 (TFOpLam (None, None, 64)     0           layer_normalization_2[0][0]      \n",
      "                                                                 dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, None, 256)    148992      dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_3 (LayerNor (None, None, 64)     128         tf.__operators__.add_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, None, 256)    296448      bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, 320)    0           layer_normalization_3[0][0]      \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_2 (MultiHe (None, None, 320)    1642560     concatenate_1[0][0]              \n",
      "                                                                 concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_4 (TFOpLam (None, None, 320)    0           concatenate_1[0][0]              \n",
      "                                                                 multi_head_attention_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_4 (LayerNor (None, None, 320)    640         tf.__operators__.add_4[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, None, 128)    41088       layer_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, None, 320)    41280       dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_5 (TFOpLam (None, None, 320)    0           layer_normalization_4[0][0]      \n",
      "                                                                 dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_5 (LayerNor (None, None, 320)    640         tf.__operators__.add_5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_3 (MultiHe (None, None, 320)    1642560     layer_normalization_5[0][0]      \n",
      "                                                                 layer_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_6 (TFOpLam (None, None, 320)    0           layer_normalization_5[0][0]      \n",
      "                                                                 multi_head_attention_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_6 (LayerNor (None, None, 320)    640         tf.__operators__.add_6[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, None, 128)    41088       layer_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, None, 320)    41280       dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_7 (TFOpLam (None, None, 320)    0           layer_normalization_6[0][0]      \n",
      "                                                                 dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_7 (LayerNor (None, None, 320)    640         tf.__operators__.add_7[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, None, 2)      642         layer_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "association (Dense)             (None, None, 250)    80250       layer_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "location (Lambda)               (None, None, 2)      0           dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "time (Dense)                    (None, None, 1)      321         layer_normalization_7[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 4,154,217\n",
      "Trainable params: 4,154,217\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/1000\n",
      " 899/2000 [============>.................] - ETA: 27:11 - loss: nan - association_loss: nan - location_loss: nan - time_loss: nan - association_nzAccuracy: 0.9805 - association_nzPrecision: nan - association_nzRecall: nan"
     ]
    }
   ],
   "source": [
    "# tf.config.threading.set_intra_op_parallelism_threads(2)\n",
    "# tf.config.threading.set_inter_op_parallelism_threads(2)\n",
    "\n",
    "trainingEvents, trainingEventList = generateEventFile(params, trainingSet=True)\n",
    "for event in trainingEventList[:,0]:\n",
    "    trainingEvents[event] = trainingEvents[event][np.where(trainingEvents[event][:,3] <= .6)[0]]\n",
    "# validationEvents, validationEventList = generateEventFile(params)\n",
    "\n",
    "generator = synthesizeEventsFromEventFile(params, trainingEvents, trainingEventList, trainingSet=False)\n",
    "# generator = synthesizeEvents(params)\n",
    "# vgen = synthesizeEventsFromEventFile(params, validationEvents, validationEventList)\n",
    "# vgen = synthesizeEvents(params)\n",
    "\n",
    "model = MatrixLink(params)\n",
    "history = model.fit(generator,\n",
    "#                  validation_data=vgen,\n",
    "                 steps_per_epoch= params['samplesPerEpoch']/params['batchSize'],\n",
    "#                  validation_steps = params['validationSamplesPerEpoch']/params['batchSize'],\n",
    "                 epochs=params['epochs'],\n",
    "                 callbacks=[saveCb(), EarlyStopping(monitor='loss', patience=50), CSVLogger('./Training/Models/IDC/logs.csv', append = True)],\n",
    "                 verbose=1)\n",
    "trainingResults(np.genfromtxt('./Training/Models/IDC/logs.csv', delimiter=',', names=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
