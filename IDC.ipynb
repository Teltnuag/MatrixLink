{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport Utils\n",
    "%aimport MatrixLinkGenerator\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "np.set_printoptions(edgeitems=30, linewidth=100000, formatter=dict(float=lambda x: \"%.3g\" % x))\n",
    "# np.set_printoptions(linewidth=np.inf)\n",
    "from obspy import UTCDateTime as dt\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from Utils import trainingResults, trainingResults2, predsMap\n",
    "# plt.rcParams['figure.figsize'] = [50, 200]\n",
    "plt.rcParams['figure.figsize'] = [16, 12]\n",
    "params = json.loads('''{\n",
    "    \"extents\": {\n",
    "        \"ak\": {\n",
    "            \"latMin\": 55.0,\n",
    "            \"latMax\": 74.0,\n",
    "            \"lonMin\": -163.0,\n",
    "            \"lonMax\": -130.0\n",
    "        },\n",
    "        \"s1\": {\n",
    "            \"latMin\": 22.0,\n",
    "            \"latMax\": 40.0,\n",
    "            \"lonMin\": 33.0,\n",
    "            \"lonMax\": 62.0\n",
    "        },\n",
    "        \"global\": {\n",
    "            \"latMin\": -90.0,\n",
    "            \"latMax\": 90.0,\n",
    "            \"lonMin\": -180.0,\n",
    "            \"lonMax\": 180.0\n",
    "        }\n",
    "    },\n",
    "    \"location\": \"global\",\n",
    "    \"maxDepth\": 50.0,\n",
    "    \"maxStationElevation\": 1.0,\n",
    "    \"trainingGeneratorSourceFile\": \"./Inputs/IDC 10-20.gz\",\n",
    "    \"trainingEventsFile\": \"./Training/Event Files/IDC 10-20 ECEF.npz\",\n",
    "    \"validationGeneratorSourceFile\": \"./Inputs/IDC 10-20.gz\",\n",
    "    \"validationEventsFile\": \"./Training/Event Files/IDC 10-20 ECEF.npz\",\n",
    "    \"arrivalProbsFile\": \"./Training/RSTT Model/S1 Dropouts.npy\",\n",
    "    \"stationFile\": \"./Archive/Stations/S1 Station List.txt\",\n",
    "    \"oneHot\": \"True\",\n",
    "    \"arrivalProbMods\": {\n",
    "        \"Pg\": 5.0,\n",
    "        \"Pn\": 3.0,\n",
    "        \"Sg\": 5.0,\n",
    "        \"Sn\": 25.0\n",
    "    },\n",
    "    \"eventsPerExample\": {\n",
    "        \"min\": 6,\n",
    "        \"max\": 20\n",
    "    },\n",
    "    \"stationsPerBatch\": {\n",
    "        \"min\": 45,\n",
    "        \"max\": 55\n",
    "    },\n",
    "    \"timeShifts\": {\n",
    "        \"min\": -0.50,\n",
    "        \"max\": 0.50\n",
    "    },\n",
    "    \"batchSize\": 1000,\n",
    "    \"samplesPerEpoch\": 1000000,\n",
    "    \"validationSamplesPerEpoch\": 250000,\n",
    "    \"epochs\": 1000,\n",
    "    \"model\": \"./Training/Models/IDC/E000 L0.0075 AL0.0045 LL1037.9866 TL0.0036 AA0.4810 AP0.8181 AR0.5462.h5\",\n",
    "    \"evalInFile\": \"./Inputs/S1 00.gz\",\n",
    "    \"evalOutFile\": \"./Training/Evaluation.gz\",\n",
    "    \"prlEvalOutFile\": \"./Training/PRL Evaluation.gz\",\n",
    "    \"maxArrivals\": 250,\n",
    "    \"minArrivals\": 5,\n",
    "    \"maxNoise\": 0.20,\n",
    "    \"clusterStrength\": 0.9,\n",
    "    \"timeNormalize\": 3600,\n",
    "    \"associationWindow\": 3600,\n",
    "    \"evalWindow\": 10.0,\n",
    "    \"phases\": {\n",
    "        \"P\": 0,\n",
    "        \"LR\": 1,\n",
    "        \"Pn\": 2,\n",
    "        \"T\": 3,\n",
    "        \"tx\": 4,\n",
    "        \"N\": 5,\n",
    "        \"Sx\": 6,\n",
    "        \"Pg\": 7,\n",
    "        \"Lg\": 8,\n",
    "        \"Sn\": 9,\n",
    "        \"S\": 10\n",
    "    },\n",
    "    \"modelArch\": {\n",
    "        \"dense\": [32, 32, 32, 64, 64],\n",
    "        \"transformers\": [128, 128],\n",
    "        \"heads\": 4,\n",
    "        \"dense2\": [128, 128, 128],\n",
    "        \"grus\": [128, 128]\n",
    "    }\n",
    "}''')\n",
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import rstt\n",
    "from copy import deepcopy\n",
    "from collections import deque\n",
    "modelPath = \"./Training/RSTT Model/pdu202009Du.geotess\"\n",
    "\n",
    "def generateEventFile(params, trainingSet = False):\n",
    "    if trainingSet:\n",
    "        eventsFile = params['trainingEventsFile']\n",
    "        generatorFile = params['trainingGeneratorSourceFile']\n",
    "    else:\n",
    "        eventsFile = params['validationEventsFile']\n",
    "        generatorFile = params['validationGeneratorSourceFile']\n",
    "    try:\n",
    "        events = np.load(eventsFile, allow_pickle=True)['events'].flatten()[0]\n",
    "        print(\"Training events loaded.\") if trainingSet else print(\"Validation events loaded.\")\n",
    "    except:\n",
    "        print(\"Events not loaded. Building from scratch.\")\n",
    "        extents = np.array(list(params['extents'][params['location']].values())+[params['maxDepth'],params['maxStationElevation']])\n",
    "        latRange = abs(extents[1] - extents[0])\n",
    "        lonRange = abs(extents[3] - extents[2])\n",
    "        timeNormalize = params['timeNormalize']\n",
    "        phases = params['phases']\n",
    "        events = {}\n",
    "        inputArrivals = pd.read_pickle(generatorFile)\n",
    "        groupedEvents = (inputArrivals.groupby('EVID').filter(lambda x: len(x) >= params['minArrivals'])).groupby('EVID')\n",
    "        count = 0\n",
    "        for eid, arrivals in groupedEvents:\n",
    "            count += 1\n",
    "            print(\"\\rBuilding event list: \" + str(count) + ' / ' + str(len(groupedEvents)), end='')\n",
    "            eventArrivals = []\n",
    "            first = dt(arrivals.TIME.min())\n",
    "            evtime = -(first - dt(arrivals.EV_TIME.min())) / timeNormalize\n",
    "            for i, arrival in arrivals.iterrows():\n",
    "                sx,sy,sz = ll2ecef(arrival.ST_LAT, arrival.ST_LON)\n",
    "                thisArrival = [sx,                                        # normalized station x\n",
    "                               sy,                                        # normalized station y\n",
    "                               sz,                                        # normalized station z\n",
    "                               ((dt(arrival.TIME)-first)/timeNormalize),  # normalized arrival time\n",
    "                               phases[arrival.PHASE],                     # phase\n",
    "                               1.,                                        # valid arrival flag\n",
    "                               5.0 / timeNormalize,                       # arrival uncertainty\n",
    "                               0.9,                                       # retention rate when dropping some arrivals\n",
    "                               abs((arrival.EV_LAT-extents[0])/latRange), # normalized event lat\n",
    "                               abs((arrival.EV_LON-extents[2])/lonRange), # normalized event lon\n",
    "                               evtime]                                    # normalized event time (relative to first arrival)\n",
    "                eventArrivals.append(thisArrival)\n",
    "            events[eid] = np.array(eventArrivals)\n",
    "        np.savez_compressed(eventsFile, events=events)\n",
    "        print()\n",
    "    eventList = list(events.keys())\n",
    "    return events, eventList\n",
    "\n",
    "def buildAssociationMatrix(evids):\n",
    "    L = np.zeros((len(evids), len(evids))) + 99\n",
    "    sparse_evids = evids[evids>=0]\n",
    "    l = np.ones((len(sparse_evids), len(sparse_evids))) * sparse_evids.reshape((-1, 1))\n",
    "    L[:len(sparse_evids), :len(sparse_evids)] = (l == l.T) * 1\n",
    "    return L\n",
    "\n",
    "def synthesizeEventsFromEventFile(params, events, eventList, trainingSet = False):\n",
    "    maxArrivals = params['maxArrivals']\n",
    "    minTimeShift = params['timeShifts']['min']\n",
    "    maxTimeShift = params['timeShifts']['max']\n",
    "    minEvents = params['eventsPerExample']['min']\n",
    "    maxEvents = params['eventsPerExample']['max']+1 # because using in np.random.randint\n",
    "    dropFactor = 0.5\n",
    "    batchSize = params['batchSize']\n",
    "\n",
    "    while True:\n",
    "        X = []\n",
    "        Y = []\n",
    "        example = 0\n",
    "        while example < batchSize:\n",
    "            #Setup - choose random events, with the first being the primary event\n",
    "            numEvents = np.random.randint(minEvents, maxEvents)\n",
    "            chosenEvents = random.sample(eventList, numEvents)\n",
    "            timeShifts = np.random.uniform(minTimeShift, maxTimeShift, size=numEvents-1)\n",
    "            for i in range(0, len(chosenEvents)):\n",
    "                thisEvent = events[chosenEvents[i]]\n",
    "                #Randomly drop some picks from the event\n",
    "                if trainingSet:\n",
    "                    drops = thisEvent[:,7]\n",
    "                    drops = drops + dropFactor*(1-drops) if dropFactor > 0 else drops*(1+dropFactor)\n",
    "                    drops = np.random.binomial(1,drops)\n",
    "                    idx = np.where(drops==1)[0]\n",
    "                    thisEvent = thisEvent[idx,:]\n",
    "                if i == 0:\n",
    "                    sequence = thisEvent\n",
    "                    sequence[:,5] = i\n",
    "                else:\n",
    "                    #Add the picks from this event\n",
    "                    currentLength = len(sequence)\n",
    "                    sequence = np.append(sequence, thisEvent, axis=0)\n",
    "                    #Shift the starting time of this event\n",
    "                    sequence[currentLength:currentLength+len(thisEvent),[3,10]] += timeShifts[i-1]\n",
    "                    sequence[currentLength:currentLength+len(thisEvent),5] = i\n",
    "\n",
    "            #Add random arrival time errors, except for the first pick of the primary event\n",
    "            if trainingSet:\n",
    "                timeShifts = np.random.uniform(-sequence[1:,6]*0.5, sequence[1:,6]*0.5)\n",
    "                sequence[1:,3] += timeShifts\n",
    "\n",
    "            #Sort by arrival time, drop picks with negative arrival times\n",
    "            idx = np.argsort(sequence[:,3])\n",
    "            remove = len(np.where((sequence[:,3] < 0))[0])\n",
    "            idx = idx[remove:]\n",
    "            sequence = sequence[idx,:]\n",
    "            if len(sequence) == 0: #We lost all the valid arrivals, so scrap this training example\n",
    "                continue\n",
    "\n",
    "            #Make labels array\n",
    "            labels = buildAssociationMatrix(sequence[:,5])\n",
    "\n",
    "            #Reset primary event times\n",
    "            ones = np.where(labels[0]==1)\n",
    "#             if len(ones[0]) == 0: #We lost all the valid arrivals, so scrap this training example\n",
    "#                 continue\n",
    "            sequence[ones,3] -= sequence[ones,3][0][0]\n",
    "            idx = np.argsort(sequence[:,3])\n",
    "\n",
    "            #Truncate picks over maximum allowed\n",
    "            idx = idx[:maxArrivals]\n",
    "            sequence = sequence[idx,:]\n",
    "            labels = labels[idx,:maxArrivals]\n",
    "\n",
    "            sequence[:,5] = 1.\n",
    "            #Pad the end if not enough picks were selected\n",
    "            padding = maxArrivals - len(sequence)\n",
    "            if padding > 0:\n",
    "                labels = np.pad(labels, (0, maxArrivals-len(labels)), constant_values=99.)\n",
    "                sequence_ = np.full((maxArrivals, 11), 0.)\n",
    "#                 sequence_[sequence.shape[0]:, [8,9]] = 99.\n",
    "                sequence_[:sequence.shape[0], :] = sequence\n",
    "                sequence = sequence_\n",
    "            X.append(sequence)\n",
    "            Y.append(labels)\n",
    "            example += 1\n",
    "            \n",
    "        #Yield these training examples\n",
    "        X = np.array(X)\n",
    "        Y = {\"association\": np.array(Y), \"location\": X[:,:,[8,9]], \"time\": X[:,:,10]}\n",
    "        X = {\"phase\": X[:,:,4], \"numerical_features\": X[:,:,[0,1,2,3,5]]}\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxArrivals = params['maxArrivals']\n",
    "from tensorflow.keras import backend as K\n",
    "from geographiclib.geodesic import Geodesic\n",
    "from tensorflow.keras.losses import binary_crossentropy as BCE\n",
    "from tensorflow.keras.metrics import binary_accuracy\n",
    "\n",
    "K.set_floatx('float64')\n",
    "maxArrivals = params['maxArrivals']\n",
    "matrixSize = maxArrivals**2\n",
    "extents = np.array(list(params['extents'][params['location']].values())+[params['maxDepth'],params['maxStationElevation']])\n",
    "latRange = abs(extents[1] - extents[0])\n",
    "lonRange = abs(extents[3] - extents[2])\n",
    "timeNormalize = params['timeNormalize']\n",
    "zero = np.float64(0)\n",
    "one = np.float64(1)\n",
    "r = np.float64(12756.2)\n",
    "nn = np.float64(99)\n",
    "\n",
    "import pymap3d as pm\n",
    "xym = 6378137.0\n",
    "xym2 = 2*xym\n",
    "zm = 6356752.3142451802\n",
    "zm2 = 2*zm\n",
    "def ll2ecef(lat,lon):\n",
    "    x,y,z = pm.geodetic2ecef(lat, lon, 0)\n",
    "    x = (x+xym)/xym2\n",
    "    y = (y+xym)/xym2\n",
    "    z = (z+zm)/zm2\n",
    "    return x,y,z\n",
    "def ecef2ll(x,y,z):\n",
    "    x = x*xym2 - xym\n",
    "    y = y*xym2 - xym\n",
    "    z = z*zm2 - zm\n",
    "    x,y,z = pm.ecef2geodetic(x,y,z)\n",
    "    return x,y\n",
    "\n",
    "def nzHaversine(y_true, y_pred):\n",
    "#     y_pred = y_pred * tf.cast(y_true != nn, tf.float64)\n",
    "#     y_true = y_true * tf.cast(y_true != nn, tf.float64)\n",
    "    observation = tf.stack([y_true[:,:,0]*latRange + extents[0], y_true[:,:,1]*lonRange + extents[2]],axis=2)*0.017453292519943295\n",
    "    prediction = tf.stack([y_pred[:,:,0]*latRange + extents[0], y_pred[:,:,1]*lonRange + extents[2]],axis=2)*0.017453292519943295\n",
    "    used = tf.reduce_sum(tf.cast(tf.greater(tf.reduce_sum(y_true, axis=2),0), dtype=tf.float64), axis=1)\n",
    "    used = tf.where(tf.equal(used, zero), one, used)\n",
    "    dlat_dlon = (observation - prediction) / 2\n",
    "    a = tf.sin(dlat_dlon[:,:,0])**2 + tf.cos(observation[:,:,0]) * tf.cos(prediction[:,:,0]) * tf.sin(dlat_dlon[:,:,1])**2\n",
    "    c = tf.asin(tf.sqrt(a))*r\n",
    "    return tf.reduce_sum((tf.reduce_sum(c, axis=1))/used) / tf.dtypes.cast(tf.shape(observation)[0], dtype=tf.float64)\n",
    "\n",
    "def nzTime(y_true, y_pred):\n",
    "    y_pred = y_pred * tf.cast(y_true != nn, tf.float64)\n",
    "    y_true = y_true * tf.cast(y_true != nn, tf.float64)\n",
    "    used = maxArrivals - tf.reduce_sum(tf.cast(tf.equal(y_true, zero), dtype=tf.float64), axis=1)\n",
    "    used = tf.where(tf.equal(used, zero), one, used)\n",
    "    diffs = tf.math.abs(tf.squeeze(y_pred)-y_true)*timeNormalize\n",
    "    diffs = (tf.squeeze(y_pred)-y_true)*timeNormalize\n",
    "    diffs = tf.reduce_sum(tf.reduce_sum(diffs, axis=1)/used)\n",
    "    return diffs/tf.dtypes.cast(tf.shape(y_true)[0], dtype= tf.float64)\n",
    "\n",
    "def nzMSE(y_true, y_pred):\n",
    "    y_pred = y_pred * tf.cast(y_true != nn, tf.float64)\n",
    "    y_true = y_true * tf.cast(y_true != nn, tf.float64)\n",
    "    used = maxArrivals - tf.reduce_sum(tf.cast(tf.equal(y_true,0), dtype=tf.float64), axis=1)\n",
    "    used = tf.where(tf.equal(used, zero), one, used)\n",
    "    return K.mean(tf.reduce_sum(K.square(tf.squeeze(y_pred)-y_true),axis=1)/used)\n",
    "\n",
    "def nzBCE(y_true, y_pred):\n",
    "    y_pred = y_pred * tf.cast(y_true != nn, tf.float64)\n",
    "    y_true = y_true * tf.cast(y_true != nn, tf.float64)\n",
    "    used = maxArrivals - tf.reduce_sum(tf.cast(tf.equal(y_true,0), dtype=tf.float64), axis=1)\n",
    "    used = tf.where(tf.equal(used, zero), one, used)\n",
    "    return K.mean(BCE(y_true, y_pred)/used)\n",
    "\n",
    "def nzAccuracy(y_true, y_pred):\n",
    "    used = matrixSize/(tf.reduce_sum(tf.cast(tf.greater(tf.reduce_sum(y_true, axis=1), zero), dtype=tf.float64), axis=1)**2)\n",
    "    used = tf.where(tf.equal(used, zero), one, used)\n",
    "    acc = tf.reduce_sum(tf.cast(y_true==tf.round(y_pred), dtype=tf.float64),axis=(1,2))/matrixSize\n",
    "    return K.mean(acc*used - used + 1)\n",
    "\n",
    "def nzRecall(y_true, y_pred):\n",
    "    y_pred = y_pred * tf.cast(y_true != nn, tf.float64)\n",
    "    y_true = y_true * tf.cast(y_true != nn, tf.float64)\n",
    "#     y_true = K.ones_like(y_true)\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    all_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    return true_positives / (all_positives + K.epsilon())\n",
    "\n",
    "def nzPrecision(y_true, y_pred):\n",
    "    y_pred = y_pred * tf.cast(y_true != nn, tf.float64)\n",
    "    y_true = y_true * tf.cast(y_true != nn, tf.float64)\n",
    "#     y_true = K.ones_like(y_true)\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    \n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    return true_positives / (predicted_positives + K.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#MatrixLinkTrainer\n",
    "import tensorflow as tf\n",
    "import absl.logging\n",
    "absl.logging.set_verbosity(absl.logging.ERROR)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.layers import Input, Embedding, Reshape, concatenate, Dense, Bidirectional, GRU, MultiHeadAttention, LayerNormalization, Lambda\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, CSVLogger\n",
    "from tensorflow.keras.backend import clip\n",
    "import logging\n",
    "import json\n",
    "\n",
    "@tf.autograph.experimental.do_not_convert\n",
    "def MatrixLink(params):\n",
    "    logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "    def buildModel(modelArch):\n",
    "        outputs = []\n",
    "        inputs = []\n",
    "        numericalInputs = Input(shape=(None,5), name='numerical_features')\n",
    "        outputs.append(numericalInputs)\n",
    "        inputs.append(numericalInputs)\n",
    "        categoricalInputs = Input(shape=(None,1), name='phase')\n",
    "        embed = Embedding(11, 4, trainable=True, embeddings_initializer=RandomNormal())(categoricalInputs)\n",
    "        embed = Reshape(target_shape=(-1, 4))(embed)\n",
    "        outputs.append(embed)\n",
    "        inputs.append(categoricalInputs)\n",
    "        outputs = concatenate(outputs)\n",
    "\n",
    "        def TransformerBlock(inputs, embed_dim, ff_dim, num_heads=2, rate=0.1, eps=1e-6):\n",
    "            attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(inputs, inputs)\n",
    "#             attn_output = Dropout(rate)(attn_output)\n",
    "            out1 = LayerNormalization(epsilon=eps)(inputs + attn_output)\n",
    "            ffn_output = Dense(ff_dim, activation=\"relu\")(out1)\n",
    "            ffn_output = Dense(embed_dim)(ffn_output)\n",
    "#             ffn_output = Dropout(rate)(ffn_output)\n",
    "            return LayerNormalization(epsilon=eps)(out1 + ffn_output) \n",
    "\n",
    "        for d1Units in modelArch['dense']:\n",
    "            outputs = Dense(units=d1Units, activation=tf.nn.relu)(outputs)\n",
    "        transformerOutputs = outputs\n",
    "        gruOutputs = outputs\n",
    "\n",
    "        for tUnits in modelArch['transformers']:\n",
    "            transformerOutputs = TransformerBlock(transformerOutputs, d1Units, tUnits, modelArch['heads'])\n",
    "        for gUnits in modelArch['grus']:\n",
    "            gruOutputs = Bidirectional(GRU(gUnits, return_sequences=True))(gruOutputs)\n",
    "\n",
    "        outputs = concatenate([transformerOutputs, gruOutputs], axis=2)\n",
    "        for tUnits in modelArch['transformers']:\n",
    "            outputs = TransformerBlock(outputs, d1Units+gUnits*2, tUnits, modelArch['heads'])\n",
    "\n",
    "        association = Dense(units=params['maxArrivals'], activation=tf.nn.sigmoid, name='association')(outputs)\n",
    "        location = Dense(units=2)(outputs)\n",
    "        location = Lambda(lambda x: clip(x, 0, 1), name='location')(location)\n",
    "        time = Dense(units=1, name='time')(outputs)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=[association, location, time])\n",
    "        losses = { 'association': nzBCE, 'location': nzHaversine, 'time': nzMSE }\n",
    "        weights = { 'association': 1.0, 'location': 0.0000025, 'time': 0.1 }\n",
    "        metrics = { 'association': [nzAccuracy, nzPrecision, nzRecall] }\n",
    "        model.compile(optimizer=Adam(clipnorm=0.00001), loss=losses, loss_weights=weights, metrics=metrics)\n",
    "        return model\n",
    "\n",
    "    model = buildModel(params['modelArch'])\n",
    "    try:\n",
    "        model.load_weights(params['model'])\n",
    "        print(\"Loaded previous weights.\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"No previous weights loaded.\")\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "class saveCb(Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.best = 100000000.\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs['loss'] < self.best:\n",
    "            self.best = logs['loss']\n",
    "            print('Saving best model with loss', self.best)\n",
    "            modelName = 'E%03d L%.4f AL%.4f LL%.4f TL%.4f AA%.4f AP%.4f AR%.4f.h5' %\\\n",
    "                (epoch, logs['loss'], logs['association_loss'], logs['location_loss'], logs['time_loss'], logs['association_nzAccuracy'], logs['association_nzPrecision'], logs['association_nzRecall'])\n",
    "            model.save(\"./Training/Models/IDC/\"+modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training events loaded.\n",
      "Unable to open file (unable to open file: name = './Training/Models/IDC/E000 L0.0075 AL0.0045 LL1037.9866 TL0.0036 AA0.4810 AP0.8181 AR0.5462.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n",
      "No previous weights loaded.\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "phase (InputLayer)              [(None, None, 1)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 1, 4)   44          phase[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "numerical_features (InputLayer) [(None, None, 5)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, None, 4)      0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, None, 9)      0           numerical_features[0][0]         \n",
      "                                                                 reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, None, 32)     320         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, None, 32)     1056        dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, None, 32)     1056        dense_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, None, 64)     2112        dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, None, 64)     4160        dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_4 (MultiHe (None, None, 64)     66368       dense_18[0][0]                   \n",
      "                                                                 dense_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_8 (TFOpLam (None, None, 64)     0           dense_18[0][0]                   \n",
      "                                                                 multi_head_attention_4[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_8 (LayerNor (None, None, 64)     128         tf.__operators__.add_8[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, None, 128)    8320        layer_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, None, 64)     8256        dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_9 (TFOpLam (None, None, 64)     0           layer_normalization_8[0][0]      \n",
      "                                                                 dense_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_9 (LayerNor (None, None, 64)     128         tf.__operators__.add_9[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_5 (MultiHe (None, None, 64)     66368       layer_normalization_9[0][0]      \n",
      "                                                                 layer_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_10 (TFOpLa (None, None, 64)     0           layer_normalization_9[0][0]      \n",
      "                                                                 multi_head_attention_5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_10 (LayerNo (None, None, 64)     128         tf.__operators__.add_10[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, None, 128)    8320        layer_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, None, 64)     8256        dense_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_11 (TFOpLa (None, None, 64)     0           layer_normalization_10[0][0]     \n",
      "                                                                 dense_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, None, 256)    148992      dense_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_11 (LayerNo (None, None, 64)     128         tf.__operators__.add_11[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, None, 256)    296448      bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, None, 320)    0           layer_normalization_11[0][0]     \n",
      "                                                                 bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_6 (MultiHe (None, None, 320)    1642560     concatenate_3[0][0]              \n",
      "                                                                 concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_12 (TFOpLa (None, None, 320)    0           concatenate_3[0][0]              \n",
      "                                                                 multi_head_attention_6[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_12 (LayerNo (None, None, 320)    640         tf.__operators__.add_12[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, None, 128)    41088       layer_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, None, 320)    41280       dense_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_13 (TFOpLa (None, None, 320)    0           layer_normalization_12[0][0]     \n",
      "                                                                 dense_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_13 (LayerNo (None, None, 320)    640         tf.__operators__.add_13[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_7 (MultiHe (None, None, 320)    1642560     layer_normalization_13[0][0]     \n",
      "                                                                 layer_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_14 (TFOpLa (None, None, 320)    0           layer_normalization_13[0][0]     \n",
      "                                                                 multi_head_attention_7[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_14 (LayerNo (None, None, 320)    640         tf.__operators__.add_14[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, None, 128)    41088       layer_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, None, 320)    41280       dense_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_15 (TFOpLa (None, None, 320)    0           layer_normalization_14[0][0]     \n",
      "                                                                 dense_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_15 (LayerNo (None, None, 320)    640         tf.__operators__.add_15[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, None, 2)      642         layer_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "association (Dense)             (None, None, 250)    80250       layer_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "location (Lambda)               (None, None, 2)      0           dense_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "time (Dense)                    (None, None, 1)      321         layer_normalization_15[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 4,154,217\n",
      "Trainable params: 4,154,217\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/1000\n",
      "1000/1000 [==============================] - 2883s 3s/step - loss: 0.0569 - association_loss: 0.0118 - location_loss: 11117.7209 - time_loss: 0.1731 - association_nzAccuracy: 0.4125 - association_nzPrecision: 0.1295 - association_nzRecall: 0.0713\n",
      "Saving best model with loss 0.04413865068464189\n",
      "Epoch 2/1000\n",
      "1000/1000 [==============================] - 2881s 3s/step - loss: 0.0281 - association_loss: 0.0087 - location_loss: 6814.5671 - time_loss: 0.0239 - association_nzAccuracy: 0.4352 - association_nzPrecision: 0.4511 - association_nzRecall: 2.3467e-04\n",
      "Saving best model with loss 0.0269031637884961\n",
      "Epoch 3/1000\n",
      "1000/1000 [==============================] - 2883s 3s/step - loss: 0.0250 - association_loss: 0.0083 - location_loss: 5987.2472 - time_loss: 0.0173 - association_nzAccuracy: 0.4352 - association_nzPrecision: 0.7163 - association_nzRecall: 0.0080\n",
      "Saving best model with loss 0.024678710598133195\n",
      "Epoch 4/1000\n",
      "1000/1000 [==============================] - 2885s 3s/step - loss: 0.0238 - association_loss: 0.0081 - location_loss: 5685.8927 - time_loss: 0.0148 - association_nzAccuracy: 0.4366 - association_nzPrecision: 0.7266 - association_nzRecall: 0.0160\n",
      "Saving best model with loss 0.02333589339314325\n",
      "Epoch 5/1000\n",
      "1000/1000 [==============================] - 2886s 3s/step - loss: 0.0210 - association_loss: 0.0081 - location_loss: 4689.2759 - time_loss: 0.0125 - association_nzAccuracy: 0.4368 - association_nzPrecision: 0.7344 - association_nzRecall: 0.0205\n",
      "Saving best model with loss 0.02032815847781061\n",
      "Epoch 6/1000\n",
      "1000/1000 [==============================] - 2885s 3s/step - loss: 0.0185 - association_loss: 0.0080 - location_loss: 3729.4436 - time_loss: 0.0114 - association_nzAccuracy: 0.4355 - association_nzPrecision: 0.7302 - association_nzRecall: 0.0203\n",
      "Saving best model with loss 0.01816959832965209\n",
      "Epoch 7/1000\n",
      "1000/1000 [==============================] - 2885s 3s/step - loss: 0.0174 - association_loss: 0.0080 - location_loss: 3356.4620 - time_loss: 0.0107 - association_nzAccuracy: 0.4373 - association_nzPrecision: 0.7363 - association_nzRecall: 0.0275\n",
      "Saving best model with loss 0.017234293695299604\n",
      "Epoch 8/1000\n",
      "1000/1000 [==============================] - 2887s 3s/step - loss: 0.0167 - association_loss: 0.0079 - location_loss: 3139.7358 - time_loss: 0.0101 - association_nzAccuracy: 0.4376 - association_nzPrecision: 0.7349 - association_nzRecall: 0.0398\n",
      "Saving best model with loss 0.016508279223825337\n",
      "Epoch 9/1000\n",
      "1000/1000 [==============================] - 2885s 3s/step - loss: 0.0161 - association_loss: 0.0078 - location_loss: 2940.8764 - time_loss: 0.0096 - association_nzAccuracy: 0.4388 - association_nzPrecision: 0.7327 - association_nzRecall: 0.0530\n",
      "Saving best model with loss 0.015936697991655053\n",
      "Epoch 10/1000\n",
      "1000/1000 [==============================] - 2886s 3s/step - loss: 0.0156 - association_loss: 0.0077 - location_loss: 2791.1528 - time_loss: 0.0091 - association_nzAccuracy: 0.4401 - association_nzPrecision: 0.7313 - association_nzRecall: 0.0637\n",
      "Saving best model with loss 0.01547844954137121\n",
      "Epoch 11/1000\n",
      "1000/1000 [==============================] - 2886s 3s/step - loss: 0.0152 - association_loss: 0.0076 - location_loss: 2670.1433 - time_loss: 0.0087 - association_nzAccuracy: 0.4398 - association_nzPrecision: 0.7305 - association_nzRecall: 0.0753\n",
      "Saving best model with loss 0.015079183579867554\n",
      "Epoch 12/1000\n",
      "1000/1000 [==============================] - 2886s 3s/step - loss: 0.0148 - association_loss: 0.0075 - location_loss: 2561.6735 - time_loss: 0.0083 - association_nzAccuracy: 0.4407 - association_nzPrecision: 0.7271 - association_nzRecall: 0.0860\n",
      "Saving best model with loss 0.014700180989242192\n",
      "Epoch 13/1000\n",
      "1000/1000 [==============================] - 2886s 3s/step - loss: 0.0145 - association_loss: 0.0075 - location_loss: 2476.7746 - time_loss: 0.0081 - association_nzAccuracy: 0.4416 - association_nzPrecision: 0.7259 - association_nzRecall: 0.0976\n",
      "Saving best model with loss 0.014434381592258204\n",
      "Epoch 14/1000\n",
      "1000/1000 [==============================] - 2888s 3s/step - loss: 0.0142 - association_loss: 0.0074 - location_loss: 2404.7362 - time_loss: 0.0078 - association_nzAccuracy: 0.4422 - association_nzPrecision: 0.7249 - association_nzRecall: 0.1082\n",
      "Saving best model with loss 0.014153291876255358\n",
      "Epoch 15/1000\n",
      "1000/1000 [==============================] - 2888s 3s/step - loss: 0.0140 - association_loss: 0.0074 - location_loss: 2336.8865 - time_loss: 0.0075 - association_nzAccuracy: 0.4424 - association_nzPrecision: 0.7242 - association_nzRecall: 0.1179\n",
      "Saving best model with loss 0.013937543093349039\n",
      "Epoch 16/1000\n",
      "1000/1000 [==============================] - 2888s 3s/step - loss: 0.0137 - association_loss: 0.0073 - location_loss: 2265.0679 - time_loss: 0.0073 - association_nzAccuracy: 0.4434 - association_nzPrecision: 0.7236 - association_nzRecall: 0.1285\n",
      "Saving best model with loss 0.013664136250023017\n",
      "Epoch 17/1000\n",
      "1000/1000 [==============================] - 2888s 3s/step - loss: 0.0136 - association_loss: 0.0073 - location_loss: 2232.3414 - time_loss: 0.0071 - association_nzAccuracy: 0.4445 - association_nzPrecision: 0.7228 - association_nzRecall: 0.1376\n",
      "Saving best model with loss 0.013504877345462454\n",
      "Epoch 18/1000\n",
      "1000/1000 [==============================] - 2886s 3s/step - loss: 0.0134 - association_loss: 0.0072 - location_loss: 2170.3719 - time_loss: 0.0070 - association_nzAccuracy: 0.4447 - association_nzPrecision: 0.7216 - association_nzRecall: 0.1453\n",
      "Saving best model with loss 0.013328206982725032\n",
      "Epoch 19/1000\n",
      "1000/1000 [==============================] - 2889s 3s/step - loss: 0.0132 - association_loss: 0.0072 - location_loss: 2124.7836 - time_loss: 0.0068 - association_nzAccuracy: 0.4452 - association_nzPrecision: 0.7212 - association_nzRecall: 0.1540\n",
      "Saving best model with loss 0.013145124234431902\n",
      "Epoch 20/1000\n",
      "1000/1000 [==============================] - 2887s 3s/step - loss: 0.0131 - association_loss: 0.0072 - location_loss: 2088.5808 - time_loss: 0.0067 - association_nzAccuracy: 0.4453 - association_nzPrecision: 0.7222 - association_nzRecall: 0.1598\n",
      "Saving best model with loss 0.013012801443876273\n",
      "Epoch 21/1000\n",
      "1000/1000 [==============================] - 2887s 3s/step - loss: 0.0129 - association_loss: 0.0071 - location_loss: 2046.4902 - time_loss: 0.0066 - association_nzAccuracy: 0.4465 - association_nzPrecision: 0.7209 - association_nzRecall: 0.1667\n",
      "Saving best model with loss 0.012859582278458256\n",
      "Epoch 22/1000\n",
      "1000/1000 [==============================] - 2887s 3s/step - loss: 0.0128 - association_loss: 0.0071 - location_loss: 2024.3197 - time_loss: 0.0064 - association_nzAccuracy: 0.4465 - association_nzPrecision: 0.7206 - association_nzRecall: 0.1735\n",
      "Saving best model with loss 0.012724758759048879\n",
      "Epoch 23/1000\n",
      "1000/1000 [==============================] - 2890s 3s/step - loss: 0.0126 - association_loss: 0.0071 - location_loss: 1950.8191 - time_loss: 0.0063 - association_nzAccuracy: 0.4463 - association_nzPrecision: 0.7205 - association_nzRecall: 0.1815\n",
      "Saving best model with loss 0.012546988261802814\n",
      "Epoch 24/1000\n",
      "1000/1000 [==============================] - 2892s 3s/step - loss: 0.0125 - association_loss: 0.0070 - location_loss: 1932.9897 - time_loss: 0.0063 - association_nzAccuracy: 0.4473 - association_nzPrecision: 0.7202 - association_nzRecall: 0.1862\n",
      "Saving best model with loss 0.012427860166469278\n",
      "Epoch 25/1000\n",
      "1000/1000 [==============================] - 2893s 3s/step - loss: 0.0123 - association_loss: 0.0070 - location_loss: 1886.4948 - time_loss: 0.0061 - association_nzAccuracy: 0.4475 - association_nzPrecision: 0.7192 - association_nzRecall: 0.1911\n",
      "Saving best model with loss 0.012276012501979922\n",
      "Epoch 26/1000\n",
      "1000/1000 [==============================] - 2895s 3s/step - loss: 0.0122 - association_loss: 0.0070 - location_loss: 1849.3848 - time_loss: 0.0060 - association_nzAccuracy: 0.4484 - association_nzPrecision: 0.7199 - association_nzRecall: 0.1990\n",
      "Saving best model with loss 0.012143267389385181\n",
      "Epoch 27/1000\n",
      "1000/1000 [==============================] - 2894s 3s/step - loss: 0.0120 - association_loss: 0.0069 - location_loss: 1803.3521 - time_loss: 0.0060 - association_nzAccuracy: 0.4492 - association_nzPrecision: 0.7210 - association_nzRecall: 0.2048\n",
      "Saving best model with loss 0.01201564507544376\n",
      "Epoch 28/1000\n",
      " 923/1000 [==========================>...] - ETA: 3:43 - loss: 0.0119 - association_loss: 0.0069 - location_loss: 1768.7878 - time_loss: 0.0059 - association_nzAccuracy: 0.4487 - association_nzPrecision: 0.7217 - association_nzRecall: 0.2099"
     ]
    }
   ],
   "source": [
    "# tf.config.threading.set_intra_op_parallelism_threads(2)\n",
    "# tf.config.threading.set_inter_op_parallelism_threads(2)\n",
    "\n",
    "trainingEvents, trainingEventList = generateEventFile(params, trainingSet=True)\n",
    "# validationEvents, validationEventList = generateEventFile(params)\n",
    "\n",
    "generator = synthesizeEventsFromEventFile(params, trainingEvents, trainingEventList, trainingSet=False)\n",
    "# generator = synthesizeEvents(params)\n",
    "# vgen = synthesizeEventsFromEventFile(params, validationEvents, validationEventList)\n",
    "# vgen = synthesizeEvents(params)\n",
    "\n",
    "model = MatrixLink(params)\n",
    "history = model.fit(generator,\n",
    "#                  validation_data=vgen,\n",
    "                 steps_per_epoch= params['samplesPerEpoch']/params['batchSize'],\n",
    "#                  validation_steps = params['validationSamplesPerEpoch']/params['batchSize'],\n",
    "                 epochs=params['epochs'],\n",
    "                 callbacks=[saveCb(), EarlyStopping(monitor='loss', patience=50), CSVLogger('./Training/Models/IDC/logs.csv', append = True)],\n",
    "                 verbose=1)\n",
    "trainingResults(np.genfromtxt('./Training/Models/IDC/logs.csv', delimiter=',', names=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MatrixLink\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from collections import deque\n",
    "from math import ceil\n",
    "from tensorflow.python.util import deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "from tensorflow.keras.models import load_model\n",
    "from obspy import UTCDateTime\n",
    "from scipy.cluster.hierarchy import ward, fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "from Utils import nzBCE, nzMSE1, nzMSE2, nzHaversine, nzAccuracy, nzPrecision, nzRecall, nzTime, evaluate\n",
    "\n",
    "# Build permutation lists and matrices to predict on\n",
    "def permute(X):\n",
    "    outerWindow = params['associationWindow']\n",
    "    minArrivals = params['minArrivals']\n",
    "    maxArrivals = params['maxArrivals']\n",
    "    edgeWindow = outerWindow/5\n",
    "    numWindows = ceil((X[:,2].max() + edgeWindow*2) / edgeWindow)\n",
    "    start = -edgeWindow\n",
    "    X_perm = [[start, start+outerWindow] for start in range(int(-edgeWindow),(numWindows-1)*int(edgeWindow),int(edgeWindow))]\n",
    "    X_perm = [[perm[0][:maxArrivals], perm[1]] for perm in [[np.where((X[:,2] >= window[0]) & (X[:,2] < window[1]))[0],window[0]] for window in X_perm] if len(perm[0]) >= minArrivals]\n",
    "    innerWindows = [window[1]+60 for window in X_perm]\n",
    "    X_perm = [perm[0] for perm in X_perm]\n",
    "    X_test = np.stack([np.concatenate((X[X_perm[perm]], np.zeros((maxArrivals - len(X_perm[perm]), 5)))) for perm in range(len(X_perm))])\n",
    "    for i in range(len(X_perm)):\n",
    "        X_test[i,:len(X_perm[i]),2] -= X_test[i,0,2]\n",
    "    X_test[:,:,2] /= params['timeNormalize']\n",
    "    return X_perm, X_test, innerWindows\n",
    "\n",
    "def buildEvents(X, labels, X_perm, X_test, Y_pred, innerWindows):\n",
    "    # Get clusters for predicted matrix at index i\n",
    "    def cluster(i):\n",
    "        valids = np.where(X_test[i][:,-1])[0]\n",
    "        validPreds = Y_pred[0][i][valids,:len(valids)]\n",
    "        L = 1-((validPreds.T + validPreds)/2)\n",
    "        np.fill_diagonal(L,0)\n",
    "        return fcluster(ward(squareform(L)), params['clusterStrength'], criterion='distance')\n",
    "\n",
    "    innerWindow = params['associationWindow'] * (3/5)\n",
    "    minArrivals = params['minArrivals']\n",
    "    catalogue = pd.DataFrame(columns=labels.columns)\n",
    "#     events = deque()\n",
    "    evid = 1\n",
    "    created = 1\n",
    "    for window in range(len(X_perm)):\n",
    "        clusters = cluster(window)\n",
    "        for c in np.unique(clusters):\n",
    "            pseudoEventIdx = np.where(clusters == c)[0]\n",
    "            pseudoEvent = X_perm[window][pseudoEventIdx]\n",
    "            if len(pseudoEvent) >= minArrivals:\n",
    "                event = X[pseudoEvent]\n",
    "                # check for containment within inner window\n",
    "                contained = (event[0,2] >= innerWindows[window]) & (event[-1,2] <= (innerWindows[window]+innerWindow))\n",
    "                if contained:\n",
    "                    candidate = labels.iloc[pseudoEvent].copy()\n",
    "                    try:\n",
    "                        candidate['ETIME'] = candidate.TIME.min() + np.median(Y_pred[3][window][pseudoEventIdx][:]*params['timeNormalize'])\n",
    "                    except:\n",
    "                        candidate['ETIME'] = -1\n",
    "                    candidate['PLAT'] = Y_pred[1][window][pseudoEventIdx][:,0]*latRange+extents[0]\n",
    "                    candidate['PLON'] = Y_pred[1][window][pseudoEventIdx][:,1]*lonRange+extents[2]\n",
    "#                     candidate['LAT'] = np.median(Y_pred[1][window][pseudoEventIdx][:,0])*latRange+extents[0]\n",
    "#                     candidate['LON'] = np.median(Y_pred[1][window][pseudoEventIdx][:,1])*lonRange+extents[2]\n",
    "                    candidate['LAT'] = np.median(candidate.PLAT)\n",
    "                    candidate['LON'] = np.median(candidate.PLON)\n",
    "                    # check for existence in catalogue\n",
    "                    overlap = candidate.ARID.isin(catalogue.ARID).sum()\n",
    "                    if overlap == 0:\n",
    "                        print(\"\\rPromoting event \" + str(created), end='')\n",
    "#                         events.append(pseudoEvent)\n",
    "                        candidate.EVID = evid\n",
    "                        catalogue = catalogue.append(candidate)\n",
    "                        evid += 1\n",
    "                        created += 1\n",
    "                    elif len(pseudoEvent) > overlap:\n",
    "                        catalogue.drop(catalogue[catalogue.ARID.isin(candidate.ARID)].index, inplace=True)\n",
    "                        candidate.EVID = evid\n",
    "                        catalogue = catalogue.append(candidate)\n",
    "                        evid += 1\n",
    "    catalogue = catalogue.groupby('EVID').filter(lambda x: len(x) >= minArrivals)\n",
    "    print()\n",
    "    return catalogue\n",
    "\n",
    "def matrixLink(X, labels, denoise=False):\n",
    "    print(\"Creating permutations... \", end='')\n",
    "    X_perm, X_test, innerWindows = permute(X)\n",
    "    print(\"\\nMaking initial predictions... \", end='')\n",
    "    Y_pred = model.predict(X_test) if oneHot else model.predict({\"phase\": X_test[:,:,3], \"numerical_features\": X_test[:,:,[0,1,2,4]]})\n",
    "    if denoise:\n",
    "        print(\"\\nEliminating noise and predicting again... \", end='')\n",
    "        for _ in range(3):\n",
    "            valids = deque()\n",
    "            for i in range(len(X_perm)):\n",
    "#                 valid = np.where(Y_pred[2][i] < 0.008)[0]\n",
    "#                 valids.append(X_perm[i][valid[valid < len(X_perm[i])]])\n",
    "                noise = np.where(Y_pred[2][i] > 0.008)[0]\n",
    "                valids.append(np.delete(X_perm[i], noise[noise < len(X_perm[i])]))\n",
    "            valids = np.array(list(set(np.concatenate(valids))))\n",
    "            X = X[valids]\n",
    "            labels = labels.iloc[valids]\n",
    "\n",
    "            X_perm, X_test, innerWindows = permute(X)\n",
    "            Y_pred = model.predict(X_test) if oneHot else model.predict({\"phase\": X_test[:,:,3], \"numerical_features\": X_test[:,:,[0,1,2,4]]})\n",
    "    print(\"clustering and building events...\")\n",
    "    catalogue = buildEvents(X, labels, X_perm, X_test, Y_pred, innerWindows)\n",
    "    return catalogue\n",
    "\n",
    "def processInput(oneHot = False):\n",
    "    print(\"Reading input file... \", end='')\n",
    "    X = []\n",
    "    labels = []\n",
    "    for i, r in inputs.iterrows(): # I can do this better\n",
    "        phase = phases[r.PHASE]\n",
    "        time = UTCDateTime(r.TIME)\n",
    "        lat = abs((r.ST_LAT - extents[0]) / latRange)\n",
    "        lon = abs((r.ST_LON - extents[2]) / lonRange)\n",
    "        otime = time - UTCDateTime(0)\n",
    "        try:\n",
    "            if oneHot:\n",
    "                arrival = [lat, lon, otime, 0, 0, 0, 0, 1]\n",
    "                arrival[3+phase] = 1\n",
    "            else:\n",
    "                arrival = [lat, lon, otime, phase, 1]\n",
    "            X.append(arrival)\n",
    "            labels.append(r)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    X = np.array(X)\n",
    "    idx = np.argsort(X[:,2])\n",
    "    X = X[idx,:]\n",
    "    X[:,2] -= X[0,2]\n",
    "    labels = pd.DataFrame([labels[i] for i in idx])\n",
    "    print(\"%d arrivals found\" % len(labels))\n",
    "    return X, labels\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pd.options.display.float_format = \"{:.2f}\".format\n",
    "#     with open(\"Parameters.json\", \"r\") as f:\n",
    "#         params = json.load(f)\n",
    "    phases = params['phases']\n",
    "    extents = np.array(list(params['extents'][params['location']].values())+[params['maxDepth'],params['maxStationElevation']])\n",
    "    latRange = abs(extents[1] - extents[0])\n",
    "    lonRange = abs(extents[3] - extents[2])\n",
    "    model = load_model(params['model'], custom_objects={'nzBCE':nzBCE, 'nzMSE':nzMSE2, 'nzMSE1':nzMSE1, 'nzMSE2':nzMSE2, 'nzHaversine':nzHaversine, 'nzPrecision':nzPrecision, 'nzRecall':nzRecall, 'nzAccuracy':nzAccuracy, 'nzTime':nzTime}, compile=True)\n",
    "\n",
    "    inFiles = ['./Inputs/S1 00.gz']\n",
    "#     inFiles = ['./Inputs/S1 50.gz', './Inputs/S1 25.gz', './Inputs/S1 15.gz', './Inputs/S1 00.gz']\n",
    "    oneHot = False\n",
    "    denoise = False\n",
    "    evals = {file:[] for file in inFiles}\n",
    "    for i in range(len(inFiles)):\n",
    "        inputs = pd.read_pickle(inFiles[i]).sort_values(by=['TIME']).reset_index(drop=True)\n",
    "        params['evalInFile'] = inFiles[i]\n",
    "#         start = inputs[inputs.TIME >= inputs.TIME.quantile(.8)].index[0]\n",
    "#         end = inputs[inputs.TIME >= inputs.TIME.quantile(.825)].index[0]\n",
    "#         inputs = inputs[start:end]\n",
    "\n",
    "        X, labels = processInput(oneHot)\n",
    "        outputs = matrixLink(X, labels, denoise)\n",
    "        outputs.to_pickle(params['evalOutFile'])\n",
    "        evals[inFiles[i]] = evaluate(params, inputs, outputs, verbose=False)\n",
    "\n",
    "    print(\"Consolidated summary for:\", params['model'])\n",
    "    print('File\\tAHM\\t Location')\n",
    "    for file in evals.keys():\n",
    "        print(file[-5:-3], \"{:8.2f}\".format(evals[file][0]), \"{:8.2f}\".format(evals[file][1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
