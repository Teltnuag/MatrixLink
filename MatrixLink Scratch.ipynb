{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport Utils\n",
    "%aimport MatrixLinkGenerator\n",
    "# import os\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "np.set_printoptions(edgeitems=30, linewidth=100000, formatter=dict(float=lambda x: \"%.3g\" % x))\n",
    "# np.set_printoptions(linewidth=np.inf)\n",
    "from obspy import UTCDateTime\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from Utils import trainingResults, trainingResults2, predsMap\n",
    "# plt.rcParams['figure.figsize'] = [50, 200]\n",
    "plt.rcParams['figure.figsize'] = [16, 12]\n",
    "params = json.loads('''{\n",
    "    \"extents\": {\n",
    "        \"ak\": {\n",
    "            \"latMin\": 55.0,\n",
    "            \"latMax\": 74.0,\n",
    "            \"lonMin\": -163.0,\n",
    "            \"lonMax\": -130.0\n",
    "        },\n",
    "        \"s1\": {\n",
    "            \"latMin\": 22.0,\n",
    "            \"latMax\": 40.0,\n",
    "            \"lonMin\": 33.0,\n",
    "            \"lonMax\": 62.0\n",
    "        }\n",
    "    },\n",
    "    \"location\": \"s1\",\n",
    "    \"maxDepth\": 50.0,\n",
    "    \"maxStationElevation\": 1.0,\n",
    "    \"trainingGeneratorSourceFile\": \"./Training/Inputs/S1 00.gz\",\n",
    "    \"trainingEventsFile\": \"./Training/Event Files/S1 Events 22-40-33-62 3+ Arrivals 120 TimeNorm.npz\",\n",
    "    \"validationGeneratorSourceFile\": \"./Inputs/S1 00.gz\",\n",
    "    \"validationEventsFile\": \"./Training/Event Files/S1 Events 22-40-33-62 3+ Arrivals 120 TimeNorm.npz\",\n",
    "    \"arrivalProbsFile\": \"./Training/RSTT Model/S1 Dropouts.npy\",\n",
    "    \"stationFile\": \"./Archive/Stations/S1 Station List.txt\",\n",
    "    \"oneHot\": \"True\",\n",
    "    \"arrivalProbMods\": {\n",
    "        \"Pg\": 5.0,\n",
    "        \"Pn\": 3.0,\n",
    "        \"Sg\": 5.0,\n",
    "        \"Sn\": 25.0\n",
    "    },\n",
    "    \"eventsPerExample\": {\n",
    "        \"min\": 1,\n",
    "        \"max\": 4\n",
    "    },\n",
    "    \"stationsPerBatch\": {\n",
    "        \"min\": 45,\n",
    "        \"max\": 55\n",
    "    },\n",
    "    \"timeShifts\": {\n",
    "        \"min\": -0.50,\n",
    "        \"max\": 0.50\n",
    "    },\n",
    "    \"batchSize\": 1000,\n",
    "    \"samplesPerEpoch\": 10000,\n",
    "    \"validationSamplesPerEpoch\": 250000,\n",
    "    \"epochs\": 2,\n",
    "    \"model\": \"./Training/Models/OneHot/E309 L0.0678 AL0.0402 LL0.0029 NL0.0853 TL0.0067 AP0.9257 AR0.9235 NP0.7388 NR0.7661 HL125.6 TL-0.288.h5\",\n",
    "    \"evalInFile\": \"./Inputs/S1 00.gz\",\n",
    "    \"evalOutFile\": \"./Training/Evaluation.gz\",\n",
    "    \"prlEvalOutFile\": \"./Training/PRL Evaluation.gz\",\n",
    "    \"maxArrivals\": 50,\n",
    "    \"minArrivals\": 5,\n",
    "    \"maxNoise\": 0.20,\n",
    "    \"clusterStrength\": 0.9,\n",
    "    \"timeNormalize\": 120,\n",
    "    \"associationWindow\": 300,\n",
    "    \"evalWindow\": 10.0,\n",
    "    \"phases\": {\n",
    "        \"Pg\": 0, \"PcP\": 0, \"Pb\": 0,\n",
    "        \"P\": 1, \"Pn\": 1,\n",
    "        \"S\": 2, \"Sg\": 2, \"ScP\": 2, \"Lg\": 2, \"Sb\": 2,\n",
    "        \"Sn\": 3\n",
    "    },\n",
    "    \"modelArch\": {\n",
    "        \"dense\": [32, 32, 64, 128, 128],\n",
    "        \"transformers\": [256, 256],\n",
    "        \"heads\": 4,\n",
    "        \"dense2\": [128, 128, 128],\n",
    "        \"grus\": [256, 256]\n",
    "    }\n",
    "}''')\n",
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#MatrixLinkTrainer\n",
    "import tensorflow as tf\n",
    "import absl.logging\n",
    "absl.logging.set_verbosity(absl.logging.ERROR)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.layers import Input, Embedding, Reshape, concatenate, Dense, Bidirectional, GRU, MultiHeadAttention, LayerNormalization\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, CSVLogger\n",
    "import logging\n",
    "import json\n",
    "from MatrixLinkGenerator import generateEventFile, synthesizeEvents, synthesizeEventsFromEventFile\n",
    "from Utils import nzBCE, nzMSE, nzPrecision, nzRecall, nzHaversine, nzTime, trainingResults\n",
    "\n",
    "@tf.autograph.experimental.do_not_convert\n",
    "def MatrixLink(params):\n",
    "    logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "    def buildModel(modelArch):\n",
    "        outputs = []\n",
    "        inputs = []\n",
    "        numericalInputs = Input(shape=(None,4), name='numerical_features')\n",
    "        outputs.append(numericalInputs)\n",
    "        inputs.append(numericalInputs)\n",
    "        categoricalInputs = Input(shape=(None,1), name='phase')\n",
    "        embed = Embedding(5, 2, trainable=True, embeddings_initializer=RandomNormal())(categoricalInputs)\n",
    "        embed = Reshape(target_shape=(-1,2))(embed)\n",
    "        outputs.append(embed)\n",
    "        inputs.append(categoricalInputs)\n",
    "        outputs = concatenate(outputs)\n",
    "\n",
    "        def TransformerBlock(inputs, embed_dim, ff_dim, num_heads=2, rate=0.1, eps=1e-6):\n",
    "            attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(inputs, inputs)\n",
    "#             attn_output = Dropout(rate)(attn_output)\n",
    "            out1 = LayerNormalization(epsilon=eps)(inputs + attn_output)\n",
    "            ffn_output = Dense(ff_dim, activation=\"relu\")(out1)\n",
    "            ffn_output = Dense(embed_dim)(ffn_output)\n",
    "#             ffn_output = Dropout(rate)(ffn_output)\n",
    "            return LayerNormalization(epsilon=eps)(out1 + ffn_output) \n",
    "\n",
    "        for d1Units in modelArch['dense']:\n",
    "            outputs = Dense(units=d1Units, activation=tf.nn.relu)(outputs)\n",
    "        transformerOutputs = outputs\n",
    "        gruOutputs = outputs\n",
    "\n",
    "        for tUnits in modelArch['transformers']:\n",
    "            transformerOutputs = TransformerBlock(transformerOutputs, d1Units, tUnits, modelArch['heads'])\n",
    "        for gUnits in modelArch['grus']:\n",
    "            gruOutputs = Bidirectional(GRU(gUnits, return_sequences=True))(gruOutputs)\n",
    "\n",
    "        outputs = concatenate([transformerOutputs, gruOutputs], axis=2)\n",
    "        for tUnits in modelArch['transformers']:\n",
    "            outputs = TransformerBlock(outputs, d1Units+gUnits*2, tUnits, modelArch['heads'])\n",
    "\n",
    "        association = Dense(units=params['maxArrivals'], activation=tf.nn.sigmoid, name='association')(outputs)\n",
    "        location = Dense(units=2, name='location')(outputs)\n",
    "        noise = Dense(units=1, activation=tf.nn.sigmoid, name='noise')(outputs)\n",
    "        time = Dense(units=1, name='time')(outputs)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=[association, location, noise, time])\n",
    "        losses = { 'association': nzBCE, 'location': nzMSE, 'noise': nzBCE, 'time': nzMSE }\n",
    "        weights = { 'association': 1.0, 'location': 1.0, 'noise': 0.25, 'time': 0.5 }\n",
    "        metrics = { 'association': [nzPrecision, nzRecall],\n",
    "                    'location': nzHaversine,\n",
    "                    'noise': [nzPrecision, nzRecall],\n",
    "                    'time': nzTime\n",
    "                  }\n",
    "        model.compile(optimizer=Adam(clipnorm=0.00001), loss=losses, loss_weights=weights, metrics=metrics)\n",
    "        return model\n",
    "\n",
    "    model = buildModel(params['modelArch'])\n",
    "    try:\n",
    "        model.load_weights(params['model'])\n",
    "        print(\"Loaded previous weights.\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"No previous weights loaded.\")\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "class saveCb(Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.best = 100.\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs['loss'] < self.best:\n",
    "            self.best = logs['loss']\n",
    "            print('Saving best model with loss', self.best)\n",
    "            modelName = 'E%03d L%.4f AL%.4f LL%.4f NL%.4f TL%.4f AP%.4f AR%.4f NP%.4f NR%.4f HL%.1f TL%.3f.h5' %\\\n",
    "                (epoch, logs['loss'], logs['association_loss'], logs['location_loss'], logs['noise_loss'], logs['time_loss'], logs['association_nzPrecision'], logs['association_nzRecall'], logs['noise_nzPrecision'], logs['noise_nzRecall'], logs['location_nzHaversine'], logs['time_nzTime'])\n",
    "            model.save(\"./Training/Models/\"+modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tf.config.threading.set_intra_op_parallelism_threads(2)\n",
    "# tf.config.threading.set_inter_op_parallelism_threads(2)\n",
    "\n",
    "# trainingEvents, trainingEventList = generateEventFile(params, trainingSet=True)\n",
    "# validationEvents, validationEventList = generateEventFile(params)\n",
    "\n",
    "# generator = synthesizeEventsFromEventFile(params, trainingEvents, trainingEventList, trainingSet=True)\n",
    "generator = synthesizeEvents(params)\n",
    "# vgen = synthesizeEventsFromEventFile(params, validationEvents, validationEventList)\n",
    "# vgen = synthesizeEvents(params)\n",
    "\n",
    "model = MatrixLink(params)\n",
    "history = model.fit(generator,\n",
    "#                  validation_data=vgen,\n",
    "                 steps_per_epoch= params['samplesPerEpoch']/params['batchSize'],\n",
    "#                  validation_steps = params['validationSamplesPerEpoch']/params['batchSize'],\n",
    "                 epochs=params['epochs'],\n",
    "                 callbacks=[saveCb(), EarlyStopping(monitor='loss', patience=40), CSVLogger('./Training/Models/logs.csv', append = True)],\n",
    "                 verbose=1)\n",
    "trainingResults(np.genfromtxt('./Training/Models/logs.csv', delimiter=',', names=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = pd.read_pickle(params['evalInFile'])\n",
    "orid = 4959179\n",
    "locations = inputs[inputs.ORID == orid][['EV_LAT', 'EV_LON', 'ST_LAT', 'ST_LON']].values\n",
    "stations = np.unique(inputs[inputs.ORID == orid][['ST_LAT', 'ST_LON']].values, axis=0)\n",
    "receivingStations = outputs[outputs.EVID == 80][['LAT', 'LON', 'ST_LAT', 'ST_LON']].values\n",
    "predsMap(locations, stations, receivingStations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MatrixLink\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from collections import deque\n",
    "from math import ceil\n",
    "from tensorflow.python.util import deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "from tensorflow.keras.models import load_model\n",
    "from obspy import UTCDateTime\n",
    "from scipy.cluster.hierarchy import ward, fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "from Utils import nzBCE, nzMSE, nzHaversine, nzAccuracy, nzPrecision, nzRecall, nzTime, evaluate\n",
    "\n",
    "# Build permutation lists and matrices to predict on\n",
    "def permute(X):\n",
    "    outerWindow = params['associationWindow']\n",
    "    minArrivals = params['minArrivals']\n",
    "    maxArrivals = params['maxArrivals']\n",
    "    edgeWindow = outerWindow/5\n",
    "    numWindows = ceil((X[:,2].max() + edgeWindow*2) / edgeWindow)\n",
    "    innerWindows = deque()\n",
    "    X_perm = deque()\n",
    "    start = -edgeWindow\n",
    "    for window in range(numWindows):\n",
    "        print('\\rCreating permutations... ' + str(window) + ' / ' + str(numWindows), end='')\n",
    "        end = start+outerWindow\n",
    "        windowArrivals = np.where((X[:,2] >= start) & (X[:,2] < end))[0]\n",
    "        start += edgeWindow\n",
    "        if len(windowArrivals) >= minArrivals:\n",
    "            X_perm.append(windowArrivals[:maxArrivals])\n",
    "            innerWindows.append(start)\n",
    "            \n",
    "            #Experimental\n",
    "#             if len(windowArrivals) > maxArrivals:\n",
    "#                 print(len(windowArrivals), end=' ')\n",
    "#                 extras = len(windowArrivals) - maxArrivals\n",
    "#                 for s in [s+1 for s in range(extras)]:\n",
    "#                     X_perm.append(windowArrivals[s:s+maxArrivals])\n",
    "#                     innerWindows.append(start)\n",
    "\n",
    "    X_test = np.zeros((len(X_perm),maxArrivals, 8 if oneHot else 5))\n",
    "    for i in range(len(X_perm)):\n",
    "        X_test[i,:len(X_perm[i])] = X[X_perm[i]]\n",
    "        X_test[i,:len(X_perm[i]),2] -= X_test[i,0,2]\n",
    "    X_test[:,:,2] /= params['timeNormalize']\n",
    "    return X_perm, X_test, innerWindows\n",
    "\n",
    "def buildEvents(X, labels, X_perm, X_test, Y_pred, innerWindows):\n",
    "    # Get clusters for predicted matrix at index i\n",
    "    def cluster(i):\n",
    "        valids = np.where(X_test[i][:,-1])[0]\n",
    "        validPreds = Y_pred[0][i][valids,:len(valids)]\n",
    "        L = 1-((validPreds.T + validPreds)/2)\n",
    "        np.fill_diagonal(L,0)\n",
    "        return fcluster(ward(squareform(L)), params['clusterStrength'], criterion='distance')\n",
    "\n",
    "    innerWindow = params['associationWindow'] * (3/5)\n",
    "    minArrivals = params['minArrivals']\n",
    "    catalogue = pd.DataFrame(columns=labels.columns)\n",
    "#     events = deque()\n",
    "    evid = 1\n",
    "    created = 1\n",
    "    for window in range(len(X_perm)):\n",
    "        clusters = cluster(window)\n",
    "        for c in np.unique(clusters):\n",
    "            pseudoEventIdx = np.where(clusters == c)[0]\n",
    "            pseudoEvent = X_perm[window][pseudoEventIdx]\n",
    "            if len(pseudoEvent) >= minArrivals:\n",
    "                event = X[pseudoEvent]\n",
    "                # check for containment within inner window\n",
    "                contained = (event[0,2] >= innerWindows[window]) & (event[-1,2] <= (innerWindows[window]+innerWindow))\n",
    "                if contained:\n",
    "                    candidate = labels.iloc[pseudoEvent].copy()\n",
    "                    candidate['ETIME'] = candidate.TIME.min() + np.median(Y_pred[3][window][pseudoEventIdx][:]*params['timeNormalize'])\n",
    "                    candidate['PLAT'] = Y_pred[1][window][pseudoEventIdx][:,0]*latRange+extents[0]\n",
    "                    candidate['PLON'] = Y_pred[1][window][pseudoEventIdx][:,1]*lonRange+extents[2]\n",
    "#                     candidate['LAT'] = np.median(Y_pred[1][window][pseudoEventIdx][:,0])*latRange+extents[0]\n",
    "#                     candidate['LON'] = np.median(Y_pred[1][window][pseudoEventIdx][:,1])*lonRange+extents[2]\n",
    "                    candidate['LAT'] = np.median(candidate.PLAT)\n",
    "                    candidate['LON'] = np.median(candidate.PLON)\n",
    "                    # check for existence in catalogue\n",
    "                    overlap = candidate.ARID.isin(catalogue.ARID).sum()\n",
    "                    if overlap == 0:\n",
    "                        print(\"\\rPromoting event \" + str(created), end='')\n",
    "#                         events.append(pseudoEvent)\n",
    "                        candidate.EVID = evid\n",
    "                        catalogue = catalogue.append(candidate)\n",
    "                        evid += 1\n",
    "                        created += 1\n",
    "                    elif len(pseudoEvent) > overlap:\n",
    "                        catalogue.drop(catalogue[catalogue.ARID.isin(candidate.ARID)].index, inplace=True)\n",
    "                        candidate.EVID = evid\n",
    "                        catalogue = catalogue.append(candidate)\n",
    "                        evid += 1\n",
    "    catalogue = catalogue.groupby('EVID').filter(lambda x: len(x) >= minArrivals)\n",
    "    print()\n",
    "    return catalogue\n",
    "\n",
    "def matrixLink(X, labels, denoise=False):\n",
    "    X_perm, X_test, innerWindows = permute(X)\n",
    "    print(\"\\nMaking initial predictions... \", end='')\n",
    "    Y_pred = model.predict(X_test) if oneHot else model.predict({\"phase\": X_test[:,:,3], \"numerical_features\": X_test[:,:,[0,1,2,4]]})\n",
    "    if denoise:\n",
    "        print(\"\\nEliminating noise and predicting again... \", end='')\n",
    "        for _ in range(3):\n",
    "            valids = deque()\n",
    "            for i in range(len(X_perm)):\n",
    "#                 valid = np.where(Y_pred[2][i] < 0.008)[0]\n",
    "#                 valids.append(X_perm[i][valid[valid < len(X_perm[i])]])\n",
    "                noise = np.where(Y_pred[2][i] > 0.008)[0]\n",
    "                valids.append(np.delete(X_perm[i], noise[noise < len(X_perm[i])]))\n",
    "            valids = np.array(list(set(np.concatenate(valids))))\n",
    "            X = X[valids]\n",
    "            labels = labels.iloc[valids]\n",
    "\n",
    "            X_perm, X_test, innerWindows = permute(X)\n",
    "            Y_pred = model.predict(X_test) if oneHot else model.predict({\"phase\": X_test[:,:,3], \"numerical_features\": X_test[:,:,[0,1,2,4]]})\n",
    "    print(\"clustering and building events...\")\n",
    "    catalogue = buildEvents(X, labels, X_perm, X_test, Y_pred, innerWindows)\n",
    "    return catalogue\n",
    "\n",
    "def processInput(oneHot = False):\n",
    "    print(\"Reading input file... \", end='')\n",
    "    X = []\n",
    "    labels = []\n",
    "    for i, r in inputs.iterrows(): # I can do this better\n",
    "        phase = phases[r.PHASE]\n",
    "        time = UTCDateTime(r.TIME)\n",
    "        lat = abs((r.ST_LAT - extents[0]) / latRange)\n",
    "        lon = abs((r.ST_LON - extents[2]) / lonRange)\n",
    "        otime = time - UTCDateTime(0)\n",
    "        try:\n",
    "            if oneHot:\n",
    "                arrival = [lat, lon, otime, 0, 0, 0, 0, 1]\n",
    "                arrival[3+phase] = 1\n",
    "            else:\n",
    "                arrival = [lat, lon, otime, phase, 1]\n",
    "            X.append(arrival)\n",
    "            labels.append(r)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    X = np.array(X)\n",
    "    idx = np.argsort(X[:,2])\n",
    "    X = X[idx,:]\n",
    "    X[:,2] -= X[0,2]\n",
    "    labels = pd.DataFrame([labels[i] for i in idx])\n",
    "    print(\"%d arrivals found\" % len(labels))\n",
    "    return X, labels\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pd.options.display.float_format = \"{:.2f}\".format\n",
    "#     with open(\"Parameters.json\", \"r\") as f:\n",
    "#         params = json.load(f)\n",
    "    phases = params['phases']\n",
    "    extents = np.array(list(params['extents'][params['location']].values())+[params['maxDepth'],params['maxStationElevation']])\n",
    "    latRange = abs(extents[1] - extents[0])\n",
    "    lonRange = abs(extents[3] - extents[2])\n",
    "    model = load_model(params['model'], custom_objects={'nzBCE':nzBCE, 'nzMSE':nzMSE, 'nzHaversine':nzHaversine, 'nzPrecision':nzPrecision, 'nzRecall':nzRecall, 'nzAccuracy':nzAccuracy, 'nzTime':nzTime}, compile=True)\n",
    "\n",
    "    inFiles = ['./Inputs/S1 00.gz']\n",
    "    inFiles = ['./Inputs/S1 50.gz', './Inputs/S1 25.gz', './Inputs/S1 15.gz', './Inputs/S1 00.gz']\n",
    "    oneHot = True\n",
    "    denoise = True\n",
    "    evals = {file:[] for file in inFiles}\n",
    "    for i in range(len(inFiles)):\n",
    "        inputs = pd.read_pickle(inFiles[i]).sort_values(by=['TIME']).reset_index(drop=True)\n",
    "        params['evalInFile'] = inFiles[i]\n",
    "        start = inputs[inputs.TIME >= inputs.TIME.quantile(.8)].index[0]\n",
    "        end = inputs[inputs.TIME >= inputs.TIME.quantile(.825)].index[0]\n",
    "        inputs = inputs[start:end]\n",
    "\n",
    "        X, labels = processInput(oneHot)\n",
    "        outputs = matrixLink(X, labels, denoise)\n",
    "        outputs.to_pickle(params['evalOutFile'])\n",
    "        evals[inFiles[i]] = evaluate(params, inputs, outputs, verbose=False)\n",
    "\n",
    "    print(\"Consolidated summary for:\", params['model'])\n",
    "    print('File\\tAHM\\t Location')\n",
    "    for file in evals.keys():\n",
    "        print(file[-5:-3], \"{:8.2f}\".format(evals[file][0]), \"{:8.2f}\".format(evals[file][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = pd.read_pickle(params['evalOutFile'])[740:770]\n",
    "locations = outputs[outputs.ORID != -1].groupby('EVID').first().reset_index()[['EV_LAT', 'EV_LON', 'LAT', 'LON']].values\n",
    "stations = np.unique(outputs[['ST_LAT', 'ST_LON']].values, axis=0)\n",
    "receivingStations = outputs[outputs.ORID != -1].groupby('EVID').head().reset_index()[['LAT', 'LON', 'ST_LAT', 'ST_LON']].values\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.io.img_tiles as cimgt\n",
    "import cartopy.feature as cfeature\n",
    "from matplotlib.colors import Normalize\n",
    "def predsMap(locations, stations, receivingStations=None, outfile=None):\n",
    "    extend = 1\n",
    "    lat_min = extents[0] - extend\n",
    "    lat_max = extents[1] + extend\n",
    "    lon_min = extents[2] - extend\n",
    "    lon_max = extents[3] + extend\n",
    "#     lat_min = locations[:,[0,2]].min() - extend\n",
    "#     lat_max = locations[:,[0,2]].max() + extend\n",
    "#     lon_min = locations[:,[1,3]].min() - extend\n",
    "#     lon_max = locations[:,[1,3]].max() + extend\n",
    "    \n",
    "    locationsR = locations*0.017453292519943295\n",
    "    dlat_dlon = (locationsR[:,[0,1]] - locationsR[:,[2,3]]) / 2\n",
    "    a = tf.sin(dlat_dlon[:,0])**2 + tf.cos(locationsR[:,0]) * tf.cos(locationsR[:,2]) * tf.sin(dlat_dlon[:,1])**2\n",
    "    diff = 2*tf.asin(tf.sqrt(a))*6378.1\n",
    "\n",
    "    cmap = plt.cm.rainbow\n",
    "    norm = Normalize(vmin=0, vmax=500)\n",
    "    colors = cmap(norm(diff))\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(25,25), subplot_kw={'projection': ccrs.PlateCarree()}, sharex=True, sharey=True)\n",
    "    states_provinces = cfeature.NaturalEarthFeature(\n",
    "        category='cultural',\n",
    "        name='admin_1_states_provinces_lines',\n",
    "        scale='50m',\n",
    "        facecolor='none')\n",
    "    ax.add_image(cimgt.Stamen('terrain-background'), 4)\n",
    "    ax.add_feature(states_provinces, edgecolor='gray')\n",
    "    ax.coastlines('110m')\n",
    "    ax.add_feature(cfeature.LAND)\n",
    "    ax.add_feature(cfeature.COASTLINE)\n",
    "    ax.add_feature(cfeature.RIVERS)\n",
    "    ax.add_feature(cfeature.LAKES)\n",
    "\n",
    "    for s in range(len(stations)):\n",
    "        ax.plot(stations[s][1], stations[s][0], 'o', markersize=7, c='k')\n",
    "    for e in range(len(locations)):\n",
    "        obs = locations[e][0:2]\n",
    "        pred = locations[e][2:4]\n",
    "        ax.plot(obs[1], obs[0], 'o', markersize=3, c='g')\n",
    "        ax.plot(pred[1], pred[0], 'o', markersize=3, c='r', alpha=0.7)\n",
    "        ax.plot([obs[1], pred[1]], [obs[0], pred[0]], color='green', alpha=0.5)\n",
    "    if receivingStations is not None:\n",
    "        for p in range(len(receivingStations)):\n",
    "            pred = receivingStations[p][0:2]\n",
    "            stas = receivingStations[p][2:4]\n",
    "            ax.plot([pred[1], stas[1]], [pred[0], stas[0]], color='red', alpha=0.5)\n",
    "    ax.set_extent([lon_min, lon_max, lat_min, lat_max])\n",
    "#     ax.legend(['Stations','Observed','Predicted'])\n",
    "\n",
    "    fig.canvas.draw()\n",
    "    fig.tight_layout(pad=0, w_pad=1, h_pad=0)\n",
    "    if outfile is not None:\n",
    "        fig.savefig(outfile)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing stuff\n",
    "# extents = np.array(list(params['extents'][params['location']].values())+[params['maxDepth'],params['maxStationElevation']])\n",
    "# latRange = abs(extents[1] - extents[0])\n",
    "# lonRange = abs(extents[3] - extents[2])\n",
    "synth = synthesizeEvents(params)\n",
    "test = next(synth)\n",
    "# total = params['batchSize']*params['maxArrivals']*params['maxArrivals']\n",
    "# padding = np.sum(test[0]['numerical_features'][:,:,3]==0)*params['maxArrivals']\n",
    "# positiveProp = np.sum(test[1]==1) / (total-padding)\n",
    "# y = np.bincount(test[0]['phase'].flatten().astype(int))\n",
    "# y[0] -= padding/params['maxArrivals']\n",
    "# ii = np.nonzero(y)\n",
    "# phaseProportions = np.vstack((ii,y[ii]/((total-padding)/params['maxArrivals']))).T\n",
    "# print(\"Total:\",total)\n",
    "# print(\"Padding:\",padding,padding/total)\n",
    "# print(\"Ones:\",np.sum(test[1]==1),positiveProp)\n",
    "# print(\"Phases:\\n\",phaseProportions)\n",
    "\n",
    "# model = load_model(params['model'], custom_objects={'nzBCE':nzBCE, 'nzMSE':nzMSE, 'nzHaversine':nzHaversine, 'nzPrecision':nzPrecision, 'nzRecall':nzRecall})\n",
    "# preds = model.predict(test[0])\n",
    "# preds[1][:,:,0] = preds[1][:,:,0]*latRange + extents[0]\n",
    "# preds[1][:,:,1] = preds[1][:,:,1]*lonRange + extents[2]\n",
    "# test[1]['location'][:,:,0] = test[1]['location'][:,:,0]*latRange + extents[0]\n",
    "# test[1]['location'][:,:,1] = test[1]['location'][:,:,1]*lonRange + extents[2]\n",
    "\n",
    "# def cluster(data, i):\n",
    "#     valids = np.where(test[0]['numerical_features'][i][:,3])[0]\n",
    "#     validPreds = data[i][valids,:len(valids)]\n",
    "#     L = 1-((validPreds.T + validPreds)/2)\n",
    "#     np.fill_diagonal(L,0)\n",
    "#     return fcluster(ward(squareform(L)), params['clusterStrength'], criterion='distance')\n",
    "\n",
    "# i=0\n",
    "# valids=np.where(test[0]['numerical_features'][i][:,3])\n",
    "# numValid=len(valids[0])\n",
    "# print(\"Arrivals:\")\n",
    "# print(test[0]['numerical_features'][i][valids])\n",
    "# print(\"Predicted clusters:\")\n",
    "# print(cluster(preds[0], i))\n",
    "# # print(\"Actual matrix:\")\n",
    "# # print(test[1]['association'][i][:numValid,:numValid])\n",
    "# print(\"Actual clusters:\")\n",
    "# print(cluster(test[1]['association'], i))\n",
    "# print(\"Predicted locations/Noise:\")\n",
    "# print(np.hstack([preds[1][i][:numValid], preds[2][i][:numValid]]))\n",
    "# print(\"Actual locations/Noise:\")\n",
    "# print(np.hstack([test[1]['location'][i][:numValid], np.expand_dims(test[1]['noise'][i][:numValid],axis=1)]))\n",
    "\n",
    "# locations = []\n",
    "# for ex in range(params['batchSize']):\n",
    "#     realClusters = cluster(test[1]['association'], ex)\n",
    "#     evids, counts = np.unique(realClusters, return_counts=True)\n",
    "#     evids = evids[np.where(counts >= params['minArrivals'])]\n",
    "#     for evid in range(len(evids)):\n",
    "#         latlons = test[1]['location'][ex][np.where(realClusters == evids[evid])]\n",
    "#         realLat, realLon = np.median(latlons[:,0]), np.median(latlons[:,1])\n",
    "#         latlons = preds[1][ex][np.where(realClusters == evids[evid])]\n",
    "#         predLat, predLon = np.median(latlons[:,0]), np.median(latlons[:,1])\n",
    "#         locations.append([realLat, realLon, predLat, predLon])\n",
    "# locations = np.array(locations)\n",
    "# stations = test[0]['numerical_features'][:,:,[0,1]].reshape(-1,2)\n",
    "# stations = np.unique(stations, axis=0)\n",
    "# stations[:,0] = stations[:,0]*latRange + extents[0]\n",
    "# stations[:,1] = stations[:,1]*lonRange + extents[2]\n",
    "\n",
    "# print(test[1]['association'][i])\n",
    "# print(np.round(preds[0][i]))\n",
    "# print(test[1]['noise'][i])\n",
    "# print(np.round(preds[2][i].T))\n",
    "# from tensorflow.keras import backend as K"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
