{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 643,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport Utils\n",
    "%aimport MatrixLinkGenerator\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "np.set_printoptions(edgeitems=30, linewidth=100000, formatter=dict(float=lambda x: \"%.3g\" % x))\n",
    "# np.set_printoptions(linewidth=np.inf)\n",
    "from obspy import UTCDateTime as dt\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from Utils import trainingResults, trainingResults2, predsMap\n",
    "# plt.rcParams['figure.figsize'] = [50, 200]\n",
    "plt.rcParams['figure.figsize'] = [16, 12]\n",
    "params = json.loads('''{\n",
    "    \"extents\": {\n",
    "        \"ak\": {\n",
    "            \"latMin\": 55.0,\n",
    "            \"latMax\": 74.0,\n",
    "            \"lonMin\": -163.0,\n",
    "            \"lonMax\": -130.0\n",
    "        },\n",
    "        \"s1\": {\n",
    "            \"latMin\": 22.0,\n",
    "            \"latMax\": 40.0,\n",
    "            \"lonMin\": 33.0,\n",
    "            \"lonMax\": 62.0\n",
    "        },\n",
    "        \"global\": {\n",
    "            \"latMin\": -90.0,\n",
    "            \"latMax\": 90.0,\n",
    "            \"lonMin\": -180.0,\n",
    "            \"lonMax\": 180.0\n",
    "        }\n",
    "    },\n",
    "    \"location\": \"global\",\n",
    "    \"maxDepth\": 50.0,\n",
    "    \"maxStationElevation\": 1.0,\n",
    "    \"trainingGeneratorSourceFile\": \"./Inputs/IDC 10-20.gz\",\n",
    "    \"trainingEventsFile\": \"./Training/Event Files/IDC 10-20 ECEF.npz\",\n",
    "    \"validationGeneratorSourceFile\": \"./Inputs/IDC 10-20.gz\",\n",
    "    \"validationEventsFile\": \"./Training/Event Files/IDC 10-20 ECEF.npz\",\n",
    "    \"arrivalProbsFile\": \"./Training/RSTT Model/S1 Dropouts.npy\",\n",
    "    \"stationFile\": \"./Archive/Stations/S1 Station List.txt\",\n",
    "    \"oneHot\": \"True\",\n",
    "    \"arrivalProbMods\": {\n",
    "        \"Pg\": 5.0,\n",
    "        \"Pn\": 3.0,\n",
    "        \"Sg\": 5.0,\n",
    "        \"Sn\": 25.0\n",
    "    },\n",
    "    \"eventsPerExample\": {\n",
    "        \"min\": 6,\n",
    "        \"max\": 20\n",
    "    },\n",
    "    \"stationsPerBatch\": {\n",
    "        \"min\": 45,\n",
    "        \"max\": 55\n",
    "    },\n",
    "    \"timeShifts\": {\n",
    "        \"min\": -0.50,\n",
    "        \"max\": 0.50\n",
    "    },\n",
    "    \"batchSize\": 100,\n",
    "    \"samplesPerEpoch\": 1000000,\n",
    "    \"validationSamplesPerEpoch\": 250000,\n",
    "    \"epochs\": 1000,\n",
    "    \"model\": \"./Training/Models/IDC/E026 L0.0120 AL0.0069 LL1796.6814 TL0.0060 AA0.4488 AP0.7215 AR0.2058.h5\",\n",
    "    \"evalInFile\": \"./Inputs/S1 00.gz\",\n",
    "    \"evalOutFile\": \"./Training/Evaluation.gz\",\n",
    "    \"prlEvalOutFile\": \"./Training/PRL Evaluation.gz\",\n",
    "    \"maxArrivals\": 250,\n",
    "    \"minArrivals\": 5,\n",
    "    \"maxNoise\": 0.20,\n",
    "    \"clusterStrength\": 0.9,\n",
    "    \"timeNormalize\": 3600,\n",
    "    \"associationWindow\": 3600,\n",
    "    \"evalWindow\": 10.0,\n",
    "    \"phases\": {\n",
    "        \"P\": 0,\n",
    "        \"LR\": 1,\n",
    "        \"Pn\": 2,\n",
    "        \"T\": 3,\n",
    "        \"tx\": 4,\n",
    "        \"N\": 5,\n",
    "        \"Sx\": 6,\n",
    "        \"Pg\": 7,\n",
    "        \"Lg\": 8,\n",
    "        \"Sn\": 9,\n",
    "        \"S\": 10\n",
    "    },\n",
    "    \"modelArch\": {\n",
    "        \"dense\": [32, 32, 32, 64, 64],\n",
    "        \"transformers\": [128, 128],\n",
    "        \"heads\": 4,\n",
    "        \"dense2\": [128, 128, 128],\n",
    "        \"grus\": [128, 128]\n",
    "    }\n",
    "}''')\n",
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import rstt\n",
    "from copy import deepcopy\n",
    "from collections import deque\n",
    "modelPath = \"./Training/RSTT Model/pdu202009Du.geotess\"\n",
    "\n",
    "def generateEventFile(params, trainingSet = False):\n",
    "    if trainingSet:\n",
    "        eventsFile = params['trainingEventsFile']\n",
    "        generatorFile = params['trainingGeneratorSourceFile']\n",
    "    else:\n",
    "        eventsFile = params['validationEventsFile']\n",
    "        generatorFile = params['validationGeneratorSourceFile']\n",
    "    try:\n",
    "        events = np.load(eventsFile, allow_pickle=True)['events'].flatten()[0]\n",
    "        print(\"Training events loaded.\") if trainingSet else print(\"Validation events loaded.\")\n",
    "    except:\n",
    "        print(\"Events not loaded. Building from scratch.\")\n",
    "        extents = np.array(list(params['extents'][params['location']].values())+[params['maxDepth'],params['maxStationElevation']])\n",
    "        latRange = abs(extents[1] - extents[0])\n",
    "        lonRange = abs(extents[3] - extents[2])\n",
    "        timeNormalize = params['timeNormalize']\n",
    "        phases = params['phases']\n",
    "        events = {}\n",
    "        inputArrivals = pd.read_pickle(generatorFile)\n",
    "        groupedEvents = (inputArrivals.groupby('EVID').filter(lambda x: len(x) >= params['minArrivals'])).groupby('EVID')\n",
    "        count = 0\n",
    "        for eid, arrivals in groupedEvents:\n",
    "            count += 1\n",
    "            print(\"\\rBuilding event list: \" + str(count) + ' / ' + str(len(groupedEvents)), end='')\n",
    "            eventArrivals = []\n",
    "            first = dt(arrivals.TIME.min())\n",
    "            evtime = -(first - dt(arrivals.EV_TIME.min())) / timeNormalize\n",
    "            for i, arrival in arrivals.iterrows():\n",
    "                sx,sy,sz = ll2ecef(arrival.ST_LAT, arrival.ST_LON)\n",
    "                ex,ey,ez = ll2ecef(arrival.EV_LAT, arrival.EV_LON)\n",
    "                thisArrival = [sx,                                        # normalized station x\n",
    "                               sy,                                        # normalized station y\n",
    "                               sz,                                        # normalized station z\n",
    "                               ((dt(arrival.TIME)-first)/timeNormalize),  # normalized arrival time\n",
    "                               phases[arrival.PHASE],                     # phase\n",
    "                               1.,                                        # valid arrival flag\n",
    "                               5.0 / timeNormalize,                       # arrival uncertainty\n",
    "                               0.9,                                       # retention rate when dropping some arrivals\n",
    "                               ex,                                        # normalized event x\n",
    "                               ey,                                        # normalized event y\n",
    "                               ez,                                        # normalized event z\n",
    "                               evtime]                                    # normalized event time (relative to first arrival)\n",
    "                eventArrivals.append(thisArrival)\n",
    "            events[eid] = np.array(eventArrivals)\n",
    "        np.savez_compressed(eventsFile, events=events)\n",
    "        print()\n",
    "    eventList = list(events.keys())\n",
    "    return events, eventList\n",
    "\n",
    "def buildAssociationMatrix(evids):\n",
    "    L = np.zeros((len(evids), len(evids))) + 99\n",
    "    sparse_evids = evids[evids>=0]\n",
    "    l = np.ones((len(sparse_evids), len(sparse_evids))) * sparse_evids.reshape((-1, 1))\n",
    "    L[:len(sparse_evids), :len(sparse_evids)] = (l == l.T) * 1\n",
    "    return L\n",
    "\n",
    "def synthesizeEventsFromEventFile(params, events, eventList, trainingSet = False):\n",
    "    maxArrivals = params['maxArrivals']\n",
    "    minTimeShift = params['timeShifts']['min']\n",
    "    maxTimeShift = params['timeShifts']['max']\n",
    "    minEvents = params['eventsPerExample']['min']\n",
    "    maxEvents = params['eventsPerExample']['max']+1 # because using in np.random.randint\n",
    "    dropFactor = 0.5\n",
    "    batchSize = params['batchSize']\n",
    "\n",
    "    while True:\n",
    "        X = []\n",
    "        Y = []\n",
    "        example = 0\n",
    "        while example < batchSize:\n",
    "            #Setup - choose random events, with the first being the primary event\n",
    "            numEvents = np.random.randint(minEvents, maxEvents)\n",
    "            chosenEvents = random.sample(eventList, numEvents)\n",
    "            timeShifts = np.random.uniform(minTimeShift, maxTimeShift, size=numEvents-1)\n",
    "            for i in range(0, len(chosenEvents)):\n",
    "                thisEvent = events[chosenEvents[i]]\n",
    "                #Randomly drop some picks from the event\n",
    "                if trainingSet:\n",
    "                    drops = thisEvent[:,7]\n",
    "                    drops = drops + dropFactor*(1-drops) if dropFactor > 0 else drops*(1+dropFactor)\n",
    "                    drops = np.random.binomial(1,drops)\n",
    "                    idx = np.where(drops==1)[0]\n",
    "                    thisEvent = thisEvent[idx,:]\n",
    "                if i == 0:\n",
    "                    sequence = thisEvent\n",
    "                    sequence[:,5] = i\n",
    "                else:\n",
    "                    #Add the picks from this event\n",
    "                    currentLength = len(sequence)\n",
    "                    sequence = np.append(sequence, thisEvent, axis=0)\n",
    "                    #Shift the starting time of this event\n",
    "                    sequence[currentLength:currentLength+len(thisEvent),[3,11]] += timeShifts[i-1]\n",
    "                    sequence[currentLength:currentLength+len(thisEvent),5] = i\n",
    "\n",
    "            #Add random arrival time errors, except for the first pick of the primary event\n",
    "            if trainingSet:\n",
    "                timeShifts = np.random.uniform(-sequence[1:,6]*0.5, sequence[1:,6]*0.5)\n",
    "                sequence[1:,3] += timeShifts\n",
    "\n",
    "            #Sort by arrival time, drop picks with negative arrival times\n",
    "            idx = np.argsort(sequence[:,3])\n",
    "            remove = len(np.where((sequence[:,3] < 0))[0])\n",
    "            idx = idx[remove:]\n",
    "            sequence = sequence[idx,:]\n",
    "            if len(sequence) == 0: #We lost all the valid arrivals, so scrap this training example\n",
    "                continue\n",
    "\n",
    "            #Make labels array\n",
    "            labels = buildAssociationMatrix(sequence[:,5])\n",
    "\n",
    "            #Reset primary event times\n",
    "            ones = np.where(labels[0]==1)\n",
    "#             if len(ones[0]) == 0: #We lost all the valid arrivals, so scrap this training example\n",
    "#                 continue\n",
    "            sequence[ones,3] -= sequence[ones,3][0][0]\n",
    "            idx = np.argsort(sequence[:,3])\n",
    "\n",
    "            #Truncate picks over maximum allowed\n",
    "            idx = idx[:maxArrivals]\n",
    "            sequence = sequence[idx,:]\n",
    "            labels = labels[idx,:maxArrivals]\n",
    "\n",
    "            sequence[:,5] = 1.\n",
    "            #Pad the end if not enough picks were selected\n",
    "            padding = maxArrivals - len(sequence)\n",
    "            if padding > 0:\n",
    "                labels = np.pad(labels, (0, maxArrivals-len(labels)), constant_values=99.)\n",
    "                sequence_ = np.full((maxArrivals, 12), 0.)\n",
    "#                 sequence_[sequence.shape[0]:, [8,9,10]] = 99.\n",
    "                sequence_[:sequence.shape[0], :] = sequence\n",
    "                sequence = sequence_\n",
    "            X.append(sequence)\n",
    "            Y.append(labels)\n",
    "            example += 1\n",
    "            \n",
    "        #Yield these training examples\n",
    "        X = np.array(X)\n",
    "        Y = {\"association\": np.array(Y), \"location\": X[:,:,[8,9,10]], \"time\": X[:,:,11]}\n",
    "        X = {\"phase\": X[:,:,4], \"numerical_features\": X[:,:,[0,1,2,3,5]]}\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxArrivals = params['maxArrivals']\n",
    "from tensorflow.keras import backend as K\n",
    "from geographiclib.geodesic import Geodesic\n",
    "from tensorflow.keras.losses import binary_crossentropy as BCE\n",
    "from tensorflow.keras.metrics import binary_accuracy\n",
    "\n",
    "K.set_floatx('float64')\n",
    "maxArrivals = params['maxArrivals']\n",
    "matrixSize = maxArrivals**2\n",
    "extents = np.array(list(params['extents'][params['location']].values())+[params['maxDepth'],params['maxStationElevation']])\n",
    "latRange = abs(extents[1] - extents[0])\n",
    "lonRange = abs(extents[3] - extents[2])\n",
    "timeNormalize = params['timeNormalize']\n",
    "zero = np.float64(0)\n",
    "one = np.float64(1)\n",
    "r = np.float64(12756.2)\n",
    "nn = np.float64(99)\n",
    "\n",
    "import pymap3d as pm\n",
    "xym = 6378137.0\n",
    "xym2 = 2*xym\n",
    "zm = 6356752.3142451802\n",
    "zm2 = 2*zm\n",
    "def ll2ecef(lat,lon):\n",
    "    x,y,z = pm.geodetic2ecef(lat, lon, 0)\n",
    "    x = (x+xym)/xym2\n",
    "    y = (y+xym)/xym2\n",
    "    z = (z+zm)/zm2\n",
    "    return x,y,z\n",
    "def ecef2ll(x,y,z):\n",
    "    x = x*xym2 - xym\n",
    "    y = y*xym2 - xym\n",
    "    z = z*zm2 - zm\n",
    "    x,y,z = pm.ecef2geodetic(x,y,z)\n",
    "    return x,y\n",
    "\n",
    "def nzHaversine(y_true, y_pred):\n",
    "#     y_pred = y_pred * tf.cast(y_true != nn, tf.float64)\n",
    "#     y_true = y_true * tf.cast(y_true != nn, tf.float64)\n",
    "    observation = tf.stack([y_true[:,:,0]*latRange + extents[0], y_true[:,:,1]*lonRange + extents[2]],axis=2)*0.017453292519943295\n",
    "    prediction = tf.stack([y_pred[:,:,0]*latRange + extents[0], y_pred[:,:,1]*lonRange + extents[2]],axis=2)*0.017453292519943295\n",
    "    used = tf.reduce_sum(tf.cast(tf.greater(tf.reduce_sum(y_true, axis=2),0), dtype=tf.float64), axis=1)\n",
    "    used = tf.where(tf.equal(used, zero), one, used)\n",
    "    dlat_dlon = (observation - prediction) / 2\n",
    "    a = tf.sin(dlat_dlon[:,:,0])**2 + tf.cos(observation[:,:,0]) * tf.cos(prediction[:,:,0]) * tf.sin(dlat_dlon[:,:,1])**2\n",
    "    c = tf.asin(tf.sqrt(a))*r\n",
    "    return tf.reduce_sum((tf.reduce_sum(c, axis=1))/used) / tf.dtypes.cast(tf.shape(observation)[0], dtype=tf.float64)\n",
    "\n",
    "def nzTime(y_true, y_pred):\n",
    "    y_pred = y_pred * tf.cast(y_true != nn, tf.float64)\n",
    "    y_true = y_true * tf.cast(y_true != nn, tf.float64)\n",
    "    used = maxArrivals - tf.reduce_sum(tf.cast(tf.equal(y_true, zero), dtype=tf.float64), axis=1)\n",
    "    used = tf.where(tf.equal(used, zero), one, used)\n",
    "    diffs = tf.math.abs(tf.squeeze(y_pred)-y_true)*timeNormalize\n",
    "    diffs = (tf.squeeze(y_pred)-y_true)*timeNormalize\n",
    "    diffs = tf.reduce_sum(tf.reduce_sum(diffs, axis=1)/used)\n",
    "    return diffs/tf.dtypes.cast(tf.shape(y_true)[0], dtype= tf.float64)\n",
    "\n",
    "def nzMSE(y_true, y_pred):\n",
    "    y_pred = y_pred * tf.cast(y_true != nn, tf.float64)\n",
    "    y_true = y_true * tf.cast(y_true != nn, tf.float64)\n",
    "    used = maxArrivals - tf.reduce_sum(tf.cast(tf.equal(y_true,0), dtype=tf.float64), axis=1)\n",
    "    used = tf.where(tf.equal(used, zero), one, used)\n",
    "    return K.mean(tf.reduce_sum(K.square(tf.squeeze(y_pred)-y_true),axis=1)/used)\n",
    "\n",
    "def nzBCE(y_true, y_pred):\n",
    "    y_pred = y_pred * tf.cast(y_true != nn, tf.float64)\n",
    "    y_true = y_true * tf.cast(y_true != nn, tf.float64)\n",
    "    used = maxArrivals - tf.reduce_sum(tf.cast(tf.equal(y_true,0), dtype=tf.float64), axis=1)\n",
    "    used = tf.where(tf.equal(used, zero), one, used)\n",
    "    return K.mean(BCE(y_true, y_pred)/used)\n",
    "\n",
    "def nzAccuracy(y_true, y_pred):\n",
    "    used = matrixSize/(tf.reduce_sum(tf.cast(tf.greater(tf.reduce_sum(y_true, axis=1), zero), dtype=tf.float64), axis=1)**2)\n",
    "    used = tf.where(tf.equal(used, zero), one, used)\n",
    "    acc = tf.reduce_sum(tf.cast(y_true==tf.round(y_pred), dtype=tf.float64),axis=(1,2))/matrixSize\n",
    "    return K.mean(acc*used - used + 1)\n",
    "\n",
    "def nzRecall(y_true, y_pred):\n",
    "    y_pred = y_pred * tf.cast(y_true != nn, tf.float64)\n",
    "    y_true = y_true * tf.cast(y_true != nn, tf.float64)\n",
    "#     y_true = K.ones_like(y_true)\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    all_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    return true_positives / (all_positives + K.epsilon())\n",
    "\n",
    "def nzPrecision(y_true, y_pred):\n",
    "    y_pred = y_pred * tf.cast(y_true != nn, tf.float64)\n",
    "    y_true = y_true * tf.cast(y_true != nn, tf.float64)\n",
    "#     y_true = K.ones_like(y_true)\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    \n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    return true_positives / (predicted_positives + K.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#MatrixLinkTrainer\n",
    "import tensorflow as tf\n",
    "import absl.logging\n",
    "absl.logging.set_verbosity(absl.logging.ERROR)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.layers import Input, Embedding, Reshape, concatenate, Dense, Bidirectional, GRU, MultiHeadAttention, LayerNormalization, Lambda\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, CSVLogger\n",
    "from tensorflow.keras.backend import clip\n",
    "import logging\n",
    "import json\n",
    "\n",
    "@tf.autograph.experimental.do_not_convert\n",
    "def MatrixLink(params):\n",
    "    logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "    def buildModel(modelArch):\n",
    "        outputs = []\n",
    "        inputs = []\n",
    "        numericalInputs = Input(shape=(None,5), name='numerical_features')\n",
    "        outputs.append(numericalInputs)\n",
    "        inputs.append(numericalInputs)\n",
    "        categoricalInputs = Input(shape=(None,1), name='phase')\n",
    "        embed = Embedding(11, 4, trainable=True, embeddings_initializer=RandomNormal())(categoricalInputs)\n",
    "        embed = Reshape(target_shape=(-1, 4))(embed)\n",
    "        outputs.append(embed)\n",
    "        inputs.append(categoricalInputs)\n",
    "        outputs = concatenate(outputs)\n",
    "\n",
    "        def TransformerBlock(inputs, embed_dim, ff_dim, num_heads=2, rate=0.1, eps=1e-6):\n",
    "            attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(inputs, inputs)\n",
    "#             attn_output = Dropout(rate)(attn_output)\n",
    "            out1 = LayerNormalization(epsilon=eps)(inputs + attn_output)\n",
    "            ffn_output = Dense(ff_dim, activation=\"relu\")(out1)\n",
    "            ffn_output = Dense(embed_dim)(ffn_output)\n",
    "#             ffn_output = Dropout(rate)(ffn_output)\n",
    "            return LayerNormalization(epsilon=eps)(out1 + ffn_output) \n",
    "\n",
    "        for d1Units in modelArch['dense']:\n",
    "            outputs = Dense(units=d1Units, activation=tf.nn.relu)(outputs)\n",
    "        transformerOutputs = outputs\n",
    "        gruOutputs = outputs\n",
    "\n",
    "        for tUnits in modelArch['transformers']:\n",
    "            transformerOutputs = TransformerBlock(transformerOutputs, d1Units, tUnits, modelArch['heads'])\n",
    "        for gUnits in modelArch['grus']:\n",
    "            gruOutputs = Bidirectional(GRU(gUnits, return_sequences=True))(gruOutputs)\n",
    "\n",
    "        outputs = concatenate([transformerOutputs, gruOutputs], axis=2)\n",
    "        for tUnits in modelArch['transformers']:\n",
    "            outputs = TransformerBlock(outputs, d1Units+gUnits*2, tUnits, modelArch['heads'])\n",
    "\n",
    "        association = Dense(units=params['maxArrivals'], activation=tf.nn.sigmoid, name='association')(outputs)\n",
    "        location = Dense(units=3)(outputs)\n",
    "        location = Lambda(lambda x: clip(x, 0, 1), name='location')(location)\n",
    "        time = Dense(units=1, name='time')(outputs)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=[association, location, time])\n",
    "        losses = { 'association': nzBCE, 'location': nzMSE, 'time': nzMSE }\n",
    "        weights = { 'association': 1.0, 'location': 1.0, 'time': 0.1 }\n",
    "        metrics = { 'association': [nzAccuracy, nzPrecision, nzRecall] }\n",
    "        model.compile(optimizer=Adam(clipnorm=0.00001), loss=losses, loss_weights=weights, metrics=metrics)\n",
    "        return model\n",
    "\n",
    "    model = buildModel(params['modelArch'])\n",
    "    try:\n",
    "        model.load_weights(params['model'])\n",
    "        print(\"Loaded previous weights.\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"No previous weights loaded.\")\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "class saveCb(Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.best = 100000000.\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs['loss'] < self.best:\n",
    "            self.best = logs['loss']\n",
    "            print('Saving best model with loss', self.best)\n",
    "            modelName = 'E%03d L%.4f AL%.4f LL%.4f TL%.4f AA%.4f AP%.4f AR%.4f.h5' %\\\n",
    "                (epoch, logs['loss'], logs['association_loss'], logs['location_loss'], logs['time_loss'], logs['association_nzAccuracy'], logs['association_nzPrecision'], logs['association_nzRecall'])\n",
    "            model.save(\"./Training/Models/IDC/\"+modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tf.config.threading.set_intra_op_parallelism_threads(2)\n",
    "# tf.config.threading.set_inter_op_parallelism_threads(2)\n",
    "\n",
    "trainingEvents, trainingEventList = generateEventFile(params, trainingSet=True)\n",
    "# validationEvents, validationEventList = generateEventFile(params)\n",
    "\n",
    "generator = synthesizeEventsFromEventFile(params, trainingEvents, trainingEventList, trainingSet=True)\n",
    "# generator = synthesizeEvents(params)\n",
    "# vgen = synthesizeEventsFromEventFile(params, validationEvents, validationEventList)\n",
    "# vgen = synthesizeEvents(params)\n",
    "\n",
    "model = MatrixLink(params)\n",
    "history = model.fit(generator,\n",
    "#                  validation_data=vgen,\n",
    "                 steps_per_epoch= params['samplesPerEpoch']/params['batchSize'],\n",
    "#                  validation_steps = params['validationSamplesPerEpoch']/params['batchSize'],\n",
    "                 epochs=params['epochs'],\n",
    "                 callbacks=[saveCb(), EarlyStopping(monitor='loss', patience=50), CSVLogger('./Training/Models/IDC/logs2.csv', append = True)],\n",
    "                 verbose=1)\n",
    "# trainingResults(np.genfromtxt('./Training/Models/IDC/logs.csv', delimiter=',', names=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading input file... 16451 arrivals found\n",
      "Creating permutations... 742 / 743\n",
      "Making initial predictions... clustering and building events...\n",
      "Promoting event 1132\n",
      "Matching event 1485 / 1485        \n",
      "Evaluating event 893 / 1485                                                                               "
     ]
    }
   ],
   "source": [
    "#MatrixLink\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from collections import deque\n",
    "from math import ceil\n",
    "from tensorflow.python.util import deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "from tensorflow.keras.models import load_model\n",
    "from obspy import UTCDateTime\n",
    "from scipy.cluster.hierarchy import ward, fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "from Utils import nzBCE, nzMSE1, nzMSE2, nzHaversine, nzAccuracy, nzPrecision, nzRecall, nzTime, evaluate\n",
    "\n",
    "# Build permutation lists and matrices to predict on\n",
    "def permute(X):\n",
    "    outerWindow = params['associationWindow']\n",
    "    minArrivals = params['minArrivals']\n",
    "    maxArrivals = params['maxArrivals']\n",
    "    edgeWindow = outerWindow/5\n",
    "    numWindows = ceil((X[:,3].max() + edgeWindow*2) / edgeWindow)\n",
    "    start = -edgeWindow\n",
    "\n",
    "    innerWindows = deque()\n",
    "    X_perm = deque()\n",
    "    for window in range(numWindows):\n",
    "        print('\\rCreating permutations... ' + str(window) + ' / ' + str(numWindows), end='')\n",
    "        end = start+outerWindow\n",
    "        windowArrivals = np.where((X[:,3] >= start) & (X[:,3] < end))[0]\n",
    "        start += edgeWindow\n",
    "        if len(windowArrivals) >= minArrivals:\n",
    "            X_perm.append(windowArrivals[:maxArrivals])\n",
    "            innerWindows.append(start)\n",
    "    X_test = np.zeros((len(X_perm),maxArrivals,6))\n",
    "    for i in range(len(X_perm)):\n",
    "        X_test[i,:len(X_perm[i])] = X[X_perm[i]]\n",
    "        X_test[i,:len(X_perm[i]),3] -= X_test[i,0,3]\n",
    "    X_test[:,:,3] /= params['timeNormalize']\n",
    "    return X_perm, X_test, innerWindows\n",
    "\n",
    "def buildEvents(X, labels, X_perm, X_test, Y_pred, innerWindows):\n",
    "    # Get clusters for predicted matrix at index i\n",
    "    def cluster(i):\n",
    "        valids = np.where(X_test[i][:,-1])[0]\n",
    "        validPreds = Y_pred[0][i][valids,:len(valids)]\n",
    "        L = 1-((validPreds.T + validPreds)/2)\n",
    "        np.fill_diagonal(L,0)\n",
    "        return fcluster(ward(squareform(L)), params['clusterStrength'], criterion='distance')\n",
    "\n",
    "    innerWindow = params['associationWindow'] * (3/5)\n",
    "    minArrivals = params['minArrivals']\n",
    "    catalogue = pd.DataFrame(columns=labels.columns)\n",
    "#     events = deque()\n",
    "    evid = 1\n",
    "    created = 1\n",
    "    for window in range(len(X_perm)):\n",
    "        clusters = cluster(window)\n",
    "        for c in np.unique(clusters):\n",
    "            pseudoEventIdx = np.where(clusters == c)[0]\n",
    "            pseudoEvent = X_perm[window][pseudoEventIdx]\n",
    "            if len(pseudoEvent) >= minArrivals:\n",
    "                event = X[pseudoEvent]\n",
    "                # check for containment within inner window\n",
    "                contained = (event[0,3] >= innerWindows[window]) & (event[-1,3] <= (innerWindows[window]+innerWindow))\n",
    "                if contained:\n",
    "                    candidate = labels.iloc[pseudoEvent].copy()\n",
    "                    try:\n",
    "                        candidate['ETIME'] = candidate.TIME.min() + np.median(Y_pred[2][window][pseudoEventIdx][:]*params['timeNormalize'])\n",
    "                    except:\n",
    "                        candidate['ETIME'] = -1\n",
    "                    candidate['PLAT'] = Y_pred[1][window][pseudoEventIdx][:,0]*latRange+extents[0]\n",
    "                    candidate['PLON'] = Y_pred[1][window][pseudoEventIdx][:,1]*lonRange+extents[2]\n",
    "#                     candidate['LAT'] = np.median(Y_pred[1][window][pseudoEventIdx][:,0])*latRange+extents[0]\n",
    "#                     candidate['LON'] = np.median(Y_pred[1][window][pseudoEventIdx][:,1])*lonRange+extents[2]\n",
    "                    candidate['LAT'] = np.median(candidate.PLAT)\n",
    "                    candidate['LON'] = np.median(candidate.PLON)\n",
    "                    # check for existence in catalogue\n",
    "                    overlap = candidate.ARID.isin(catalogue.ARID).sum()\n",
    "                    if overlap == 0:\n",
    "                        print(\"\\rPromoting event \" + str(created), end='')\n",
    "#                         events.append(pseudoEvent)\n",
    "                        candidate.EVID = evid\n",
    "                        catalogue = catalogue.append(candidate)\n",
    "                        evid += 1\n",
    "                        created += 1\n",
    "                    elif len(pseudoEvent) > overlap:\n",
    "                        catalogue.drop(catalogue[catalogue.ARID.isin(candidate.ARID)].index, inplace=True)\n",
    "                        candidate.EVID = evid\n",
    "                        catalogue = catalogue.append(candidate)\n",
    "                        evid += 1\n",
    "    catalogue = catalogue.groupby('EVID').filter(lambda x: len(x) >= minArrivals)\n",
    "    print()\n",
    "    return catalogue\n",
    "\n",
    "def matrixLink(X, labels, denoise=False):\n",
    "    print(\"Creating permutations... \", end='')\n",
    "    X_perm, X_test, innerWindows = permute(X)\n",
    "    print(\"\\nMaking initial predictions... \", end='')\n",
    "    Y_pred = model.predict({\"phase\": X_test[:,:,4], \"numerical_features\": X_test[:,:,[0,1,2,3,5]]})\n",
    "    if denoise:\n",
    "        print(\"\\nEliminating noise and predicting again... \", end='')\n",
    "        for _ in range(3):\n",
    "            valids = deque()\n",
    "            for i in range(len(X_perm)):\n",
    "                noise = np.where(Y_pred[2][i] > 0.008)[0]\n",
    "                valids.append(np.delete(X_perm[i], noise[noise < len(X_perm[i])]))\n",
    "            valids = np.array(list(set(np.concatenate(valids))))\n",
    "            X = X[valids]\n",
    "            labels = labels.iloc[valids]\n",
    "            X_perm, X_test, innerWindows = permute(X)\n",
    "            Y_pred = model.predict({\"phase\": X_test[:,:,4], \"numerical_features\": X_test[:,:,[0,1,2,3,5]]})\n",
    "    print(\"clustering and building events...\")\n",
    "    catalogue = buildEvents(X, labels, X_perm, X_test, Y_pred, innerWindows)\n",
    "    return catalogue\n",
    "\n",
    "def processInput():\n",
    "    print(\"Reading input file... \", end='')\n",
    "    X = []\n",
    "    labels = []\n",
    "    for i, r in inputs.iterrows(): # I can do this better\n",
    "        phase = phases[r.PHASE]\n",
    "        time = UTCDateTime(r.TIME)\n",
    "        lat = abs((r.ST_LAT - extents[0]) / latRange)\n",
    "        lon = abs((r.ST_LON - extents[2]) / lonRange)\n",
    "        x,y,z = ll2ecef(lat,lon)\n",
    "        otime = time - UTCDateTime(0)\n",
    "        try:\n",
    "            arrival = [x, y, z, otime, phase, 1]\n",
    "            X.append(arrival)\n",
    "            labels.append(r)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    X = np.array(X)\n",
    "    idx = np.argsort(X[:,3])\n",
    "    X = X[idx,:]\n",
    "    X[:,3] -= X[0,3]\n",
    "    labels = pd.DataFrame([labels[i] for i in idx])\n",
    "    print(\"%d arrivals found\" % len(labels))\n",
    "    return X, labels\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pd.options.display.float_format = \"{:.2f}\".format\n",
    "#     with open(\"Parameters.json\", \"r\") as f:\n",
    "#         params = json.load(f)\n",
    "    phases = params['phases']\n",
    "    extents = np.array(list(params['extents'][params['location']].values())+[params['maxDepth'],params['maxStationElevation']])\n",
    "    latRange = abs(extents[1] - extents[0])\n",
    "    lonRange = abs(extents[3] - extents[2])\n",
    "    model = load_model(params['model'], custom_objects={'nzBCE':nzBCE, 'nzMSE':nzMSE2, 'nzMSE1':nzMSE1, 'nzMSE2':nzMSE2, 'nzHaversine':nzHaversine, 'nzPrecision':nzPrecision, 'nzRecall':nzRecall, 'nzAccuracy':nzAccuracy, 'nzTime':nzTime}, compile=True)\n",
    "\n",
    "    inFiles = ['./Inputs/IDC Test.gz']\n",
    "    inFiles = ['./Inputs/IDC 10-20.gz']\n",
    "    denoise = False\n",
    "    evals = {file:[] for file in inFiles}\n",
    "    for i in range(len(inFiles)):\n",
    "        inputs = pd.read_pickle(inFiles[i]).sort_values(by=['TIME']).reset_index(drop=True)\n",
    "        params['evalInFile'] = inFiles[i]\n",
    "        start = inputs[inputs.TIME >= inputs.TIME.quantile(.8)].index[0]\n",
    "        end = inputs[inputs.TIME >= inputs.TIME.quantile(.802)].index[0]\n",
    "        inputs = inputs[start:end]\n",
    "\n",
    "        X, labels = processInput()\n",
    "        outputs = matrixLink(X, labels, denoise)\n",
    "        outputs.to_pickle(params['evalOutFile'])\n",
    "        evals[inFiles[i]] = evaluate(params, inputs, outputs, verbose=False)\n",
    "\n",
    "    print(\"Consolidated summary for:\", params['model'])\n",
    "    print('File\\tAHM\\t Location')\n",
    "    for file in evals.keys():\n",
    "        print(file[-5:-3], \"{:8.2f}\".format(evals[file][0]), \"{:8.2f}\".format(evals[file][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  1  1  2  1  1  1  2  2  1  2  2  2  3  3  3  2  3  3  3  2  1  4  5  5  6  1  4  6  6  4  4  4  4  4  4  5  5  4  4  4  5  7  7  1  8  7  7  8  4  7  8  1  8  9  9  9  1  9  1 10 10 11 11 12 10 12 10 11 11 13 12 13 12  1 12 13 13 13  1 14 14 15  4 16 14 17 16 15 16 16 16 16 16 17 14 14 16 16 15 17 17]\n",
      "[1 1 1 2 1 1 1 2 2 1 2 2 2 3 3 3 2 3 3 3 2 1 4 5 5 2 1 4 2 2 4 4 4 4 4 5 5 5 4 4 4 5 6 6 1 6 6 6 6 5 6 6 1 6 7 7 7 1 7 1 7 7 7 7 8 7 8 7 7 7 8 8 8 8 1 8 8 8 8 1 9 9 9 5 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9]\n",
      "(array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17]), array([15,  8,  6, 13,  5,  3,  5,  4,  4,  4,  4,  5,  5,  5,  3,  9,  4]))\n",
      "(array([1, 2, 3, 4, 5, 6, 7, 8, 9]), array([15, 11,  6, 10,  8,  9, 12, 10, 21]))\n"
     ]
    }
   ],
   "source": [
    "# event = outputs[outputs.EVID == 40]\n",
    "# dists = haversine(event.PLAT, event.PLON, event.LAT.iloc[0], event.LON.iloc[0])\n",
    "# print(event.iloc[np.where(dists < (dists.mean()+dists.std()))[0]])\n",
    "\n",
    "def cluster(i):\n",
    "    valids = np.where(X_test[i][:,-1])[0]\n",
    "    validPreds = Y_pred[0][i][valids,:len(valids)]\n",
    "    L = 1-((validPreds.T + validPreds)/2)\n",
    "    np.fill_diagonal(L,0)\n",
    "    return fcluster(ward(squareform(L)), 0.9, criterion='distance')\n",
    "associationWindow = 3600\n",
    "i=6\n",
    "evids = cluster(i)\n",
    "evids = [evids[index] for index in sorted(np.unique(evids, return_index=True)[1])]\n",
    "nevids = {evids[e]:e+1 for e in range(len(evids))}\n",
    "evids = cluster(i)\n",
    "predicted = np.array([nevids[evid] for evid in evids])\n",
    "\n",
    "window = inputs[(inputs.TIME >= inputs.TIME.min() + innerWindows[i] - associationWindow*1/5) & (inputs.TIME < inputs.TIME.min() + innerWindows[i] + associationWindow*4/5)].EVID\n",
    "evids = window.unique()\n",
    "evids = [evids[index] for index in sorted(np.unique(evids, return_index=True)[1])]\n",
    "nevids = {evids[e]:e+1 for e in range(len(evids))}\n",
    "labels = np.array([nevids[evid] for evid in window.values])\n",
    "print(labels)\n",
    "print(predicted)\n",
    "print(np.unique(labels, return_counts=True))\n",
    "print(np.unique(predicted, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56.43 -58.56\n",
      "0.644548535338 0.263562816995 0.916176860751\n",
      "(56.430000000000007, -58.560000000000024)\n"
     ]
    }
   ],
   "source": [
    "import pymap3d as pm\n",
    "xym = 6378137.0\n",
    "xym2 = 2*xym\n",
    "zm = 6356752.3142451802\n",
    "zm2 = 2*zm\n",
    "def ll2ecef(lat,lon):\n",
    "    x,y,z = pm.geodetic2ecef(lat, lon, 0)\n",
    "    x = (x+xym)/xym2\n",
    "    y = (y+xym)/xym2\n",
    "    z = (z+zm)/zm2\n",
    "    return x,y,z\n",
    "def ecef2ll(x,y,z):\n",
    "    x = x*xym2 - xym\n",
    "    y = y*xym2 - xym\n",
    "    z = z*zm2 - zm\n",
    "    x,y,z = pm.ecef2geodetic(x,y,z)\n",
    "    return x,y\n",
    "\n",
    "lat, lon = 56.43, 58.56\n",
    "x,y,z = ll2ecef(lat, lon)\n",
    "print(lat, lon)\n",
    "print(x,y,z)\n",
    "print(ecef2ll(x,y,z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import load_model\n",
    "# idc = pd.read_pickle(\"./Inputs/IDC 10-20.gz\")\n",
    "# idc = idc.drop_duplicates('ARID').sort_values(by='TIME')\n",
    "# idc = idc[idc.IPHASE.isin(idc.IPHASE.value_counts()[0:11].keys())]\n",
    "# window = 3600\n",
    "# start = idc.TIME.min()\n",
    "# counts = []\n",
    "# for h in range(int((idc.TIME.max() - start) / window)+1):\n",
    "#     counts.append(len(idc[(idc.TIME >= start) & (idc.TIME < start+window)]))\n",
    "#     start += window\n",
    "# counts = np.array(counts)\n",
    "# counts.max()\n",
    "# generator = synthesizeEventsFromEventFile(params, trainingEvents, trainingEventList, trainingSet=True)\n",
    "# test = next(generator)\n",
    "# total = params['batchSize']*params['maxArrivals']*params['maxArrivals']\n",
    "# padding = np.sum(test[0]['numerical_features'][:,:,3]==0)*params['maxArrivals']\n",
    "# positiveProp = np.sum(test[1]==1) / (total-padding)\n",
    "# y = np.bincount(test[0]['phase'].flatten().astype(int))\n",
    "# y[0] -= padding/params['maxArrivals']\n",
    "# ii = np.nonzero(y)\n",
    "# phaseProportions = np.vstack((ii,y[ii]/((total-padding)/params['maxArrivals']))).T\n",
    "# print(\"Total:\",total)\n",
    "# print(\"Padding:\",padding,padding/total)\n",
    "# print(\"Ones:\",np.sum(test[1]==1),positiveProp)\n",
    "# print(\"Phases:\\n\",phaseProportions)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
